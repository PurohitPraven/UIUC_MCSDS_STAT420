---
title: 'Final Project'
author: "STAT 420, Summer 2020, Mandalorians"
date: ''
output:
   
  bookdown::html_document2: 
    toc: true
    number_sections: yes
    fig_caption: yes
    urlcolor: cyan
    toc_depth: 4
---

\usepackage{subfig}
\DeclareUnicodeCharacter{00A0}{~}

***


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
```


```{r message=FALSE, include=FALSE}

library("tibble")
library("readr")
library("Hmisc")
library("dplyr")
library("statsr")
library("GGally")
library("gridExtra")
library("MASS")
library("faraway")
library("lmtest")
library("stringr")
```

# Introduction

### Title of Project

The title of our project is "House Price Prediction"

### Source of dataset

The dataset is from Kaggle. Link is https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data

### Statement of personal interest

Personal Interest -  We wish to start on Kaggle competitions and this is supposed to be a good kickstarter dataset. We intend to submit this on Kaggle post completion of the project

### Description of the dataset

We have a housing dataset. We have pulled this dataset as part of a kaggle challenge. The dataset is split into training data with 1460 observation and test data which is 1459 observations. Since this is a Kaggle challenge, we dont have the predicted values for the test dataset, hence we cant use the test data to validate our predictions.


### What are the variables?

The variables of this dataset represent the characteristics of a house. It includes things like, zone, neighborhood, size, shape, condition, year etc. It is an exhaustive compilation of all the potential things that can impact the price of a house.

We unfortunately do not have a file describing each variable, but looking at them it is easy to infer their meaning.

### Goal of this model

The goal of the model we are creating is to accurately predict the price of the houses.

### Dataset structure

We can see the structure of the dataset below

```{r}
df1 <-read.csv("train.csv",as.is = FALSE )

str(df1)
```

# Method

## Data Cleaning


### Investigations - Visually looking at data statistics {#id1}


Now we investigate the dataset to see what actions we need to take with the dataset before creating a model

```{r message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}
Hmisc::html(Hmisc::describe(df1))
```

Since the output is very long, we use echo=FALSE, but as a sample, we show the output below for the first 5 rows

```{r message=FALSE, warning=FALSE}
Hmisc::html(Hmisc::describe(df1[,1:5]), where=c('cwd', 'tmp'))
```



### Data Cleanup based on data inspection

We have 80 predictors in the dataset. That tells us that we really arent short of predictors.

Based on the above dataset description (showing only first 5 for brevity) we draw the following conclusions


**Remove these columns**

* Remove Id column since it is not a predictor and is simply a running id
* Alley - has 1369 missing values and only 91 observations with values. 
* Street - Only 6 elements or 0.4% data elements have Street as Grvl, rest are all Pave. 6 is too few data elements to make a prediction 
* LotFrontage has 259 missing values. This is about 18% of total observations. If we were short of predictors we would not delete it. Since there a lot of predictors that convey space of the house, we will delete this column
* Utilities - Only 1 value has NoSeWa and the rest are all AllPub. Since only 1 element isnt enough for creating a model
* Condition2 - remove since 99% are Norm while the remaining 1% are distributed across 7 other values. This implies that the other 7 values dont have enough data to make a useful prediction
* RoofMatl - 98.2% is CompShg and the remaining 1.8% is spread across 7 values and the remaining 7 values dont have enough data to make a prediction
* X3SsnPorch - 98.4% is value 0, and the remaining 1.6% is spread across 19 types. Each of these has only 1 or 2 observations hence not enough data for prediction
* PoolArea - 99.5% has value 0 and the remaining .5% is spread across 7 factors, implying not enough data in each type to make a model
* PoolQC - since only 7 observations have non missing values
* MiscFeature, MiscVal - since only 54 and 52 observations respectively  have values
* FirePlaceQu - has 690 values missing which is approximately 47% of total predictors. Since we are not short of number of predictors, and this one has half the values missing, we will delete this predictor
* Fence - 1179 values missing.


**Other Observations**

* MasVnrType and MasVnrArea - has 8 missing values
* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2 - each have 37 or 38 missing values
* Electrical = has 1 value missing
* GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond - has 81 values missing

Based on the above analysis we remove the ones that we want to remove and store the column names for the ones with missing values in a vector to be used later if necessary

```{r}
remove_cols <- c("Id", "Alley", "Street", "LotFrontage", "Utilities", "Condition2", "RoofMatl", "X3SsnPorch", "PoolArea", "PoolQC", "MiscFeature", "MiscVal", "FireplaceQu", "Fence")

col_with_missing_values <- c("MasVnrType", "MasVnrArea", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "Electrical", "GarageType", "GarageYrBlt", "GarageFinish", "GarageQual", "GarageCond")

keep_cols <- colnames(df1)[!(colnames(df1) %in% remove_cols)]

df_cols_removed <- subset(df1, select = as.vector(eval(keep_cols)))
```



### Investigation - Correlation

Now, to identify columns that have high correlations. We define high correlation as great than 70% correlation.

In order to identify correlation we first create a dataframe that has only numeric predictors

```{r}
df_only_numeric <- df_cols_removed[,sapply(df_cols_removed, is.numeric)]
```

Now we identify the correlation in numeric predictors. We mark all correlations less than .75 as NA, so that we can easily identify highly correlated features

For brevity of the report, we only show the head of the correlation matrix, while we had looked at the entire matrix to come to our conclusions below

```{r}
cor_relation = cor(df_only_numeric, use = "complete.obs")
cor_relation[abs(cor_relation) < 0.75] <- NA
head(cor_relation)
```



### Data Cleanup based on correlation

Based on the above correlation matrix, we make the below observations

**Remove the following predictors**

* YearBuilt and GarageYrBlt has .82 correlation coefficient. This makes sense since the Garage will be built in the same year as the house, with a few exceptions. We also reason that the price will be more tied to how old the house is. Since YearBuilt of house will be a more useful predictor, we will remove GarageYrBlt
* X1stFlrSF and TotalBsmtSF has a .82 correlation. This is obvious since the size of the 1st floor will be very similar to size of basement. We can also reason that the price will be more correlated to the price of the 1st floor. Hence we will remove TotalBsmtSF
* GarageArea and GarageCars have a .83 correlation. Intuitively we can see that both of these are related. Hence we remove GarageCars 
* GrLivArea and TotRmsAbvGrd has a correlation of .82. We see that number of Bedrooms and kitchens and bathrooms are already captured separately. Hence we remove TotRmsAbvGrd

**Make a note of these**

* OverallQual has a .78 correlation with SalePrice. Given that, we should make sure OverallQual is part of the model



```{r}
remove_cols2 <- c("GarageYrBlt", "TotalBsmtSF", "GarageCars", "TotRmsAbvGrd")

keep_cols2 <- colnames(df_cols_removed)[!(colnames(df_cols_removed) %in% remove_cols2)]

df_cols_removed <- subset(df_cols_removed, select = as.vector(eval(keep_cols2)))

ncol(df_cols_removed)

```

We see that now we are left with 63 columns compared to the 81 that we started with.



### Data Cleanup to handle missing values

We know that step wont work with missing values. Hence we will need to remove those. 

```{r}
df_cols_removed <- na.omit(df_cols_removed)
nrow(df_cols_removed)
```

We started with 1460 observations and 81 columns and after our initial cleanup we end up with 1338 observations and 63 columns



## Transformation identification



### Pairs plot

Now, before we begin modeling, we look at the pairs plots to see if any of the parameters are an obvious choice for transformations

For the purpose of being able to see the plots clearly, we do two things for the visual of the pair plot

1) We only look at numeric columns
2) We filter out the columns that have 25 or less discrete values, since they will probably not be candidates for transformations

We wont save this interim dataset. It is only created to be able to see the distribution across many predictors clearly

```{r}
df_only_numeric <- df_cols_removed[,sapply(df_cols_removed, is.numeric)]
remove_for_plot <- c()
j=1
for(i in colnames(df_only_numeric)){
  k <- length(unique(df_only_numeric[,i])<20)
  if(k<20){
    remove_for_plot[j] <- i
    j <- j+1
  }
}

keep_for_plot <- colnames(df_only_numeric)[!(colnames(df_only_numeric) %in% remove_for_plot)]

df_cols_removed_for_plot <- subset(df_only_numeric, select = as.vector(eval(keep_for_plot)))

```



```{r fig.height=30, fig.width=35, message=FALSE, warning=FALSE}
#pairs(df_only_numeric)
ggpairs( df_cols_removed_for_plot, ggplot2::aes(color=I("navyblue")), title = "ggpairs plot to see correlation and distribution", lower = list(continuous = wrap("smooth")), axisLabels = "show", switch = "both")
```

We make the following observations from the plot

**Potential for transformations**

* LotArea
* MasVnrArea
* BsmtFinSF1
* BsmtFinSF2
* X1stFlrSF
* GrLivArea
* GarageArea
* WoodDeckSF
* OpenPorchSF
* EnclosedPorch
* YearBuilt
* YearRemodAdd 
* SalePrice


```{r}
diagnostics <- function(model = fit_1, pcol = 'dodgerblue', lcol = 'red', alpha = .05, plotit = TRUE, testit = TRUE){
  
  if(plotit == TRUE){
    g1 <- ggplot(data = model, aes(sample=.resid)) + 
      stat_qq(color=I(pcol)) + stat_qq_line(color = I(lcol)) +
      ggtitle("Normal QQ Plot") +  theme_light() 
    
    g2 <- ggplot(data = model, aes(x = fitted(model), y = resid(model))) +
      geom_point(color=I(pcol)) + geom_hline(yintercept=0, color = I(lcol)) +
      xlab("Fitted") + ylab("Residuals") + ggtitle("Residuals vs Fitted Plot") + theme_light() 
    
    grid.arrange(g1, g2, ncol=2)
  }
  
  if(testit == TRUE){
    shapiro_Normalcy_test_result <- shapiro.test(resid(model))$"p.value"
    
    bptest_Const_Variance_test_result <-  bptest(model)$"p.value"[[1]]
    
    rmse <- round(sqrt(mean(resid(model) ^ 2)), 4)
    aic <- extractAIC(model)[2]
    num_predictors <- num_predictors_in_formula(formula(model))
    
    l1 <- list(num_predictors=num_predictors, shapiro_Normalcy_test_pvalue=shapiro_Normalcy_test_result, bptest_Const_Variance_test_pvalue=bptest_Const_Variance_test_result, RMSE=rmse, AdjustedR2=summary(model)$"adj.r.squared", AIC=aic)
    
    return(l1)
  }
}
```

```{r}
create_formula <- function(data_set, response, cols_to_remove="", cols_to_add=""){
  
  predictor_list <- colnames(df_cols_removed)
  
  predictor_list <- predictor_list[!(predictor_list %in% cols_to_remove)]
  n <- length(predictor_list)

  for(i in 1:length(cols_to_add)){
    n <- n+1
    predictor_list[n] <- cols_to_add[i]
  }
  
  frm1 <- paste(response, " ~ ", paste(predictor_list, collapse = ' + '))
}
```

```{r}
num_predictors_in_formula <- function(model_formula){

  return(length(strsplit(as.character(model_formula)[3], fixed = TRUE, split = "+")[[1]]))
}

```


In order to validate that transformations are necessary we will start with a simple additive model and look at its diagnostics plots

```{r fig.height=5, fig.width=15, message=FALSE, warning=FALSE}
m1 <- lm("SalePrice~.", data=df_cols_removed)
knitr::kable(data.frame(diagnostics(m1)))
```


Based on the diagnostics and the plots we see that some kind of transformation for the response is necessary.



### Boxcox lambda identifications for response and predictors {#id2}



#### Response transformation identification

In order to figure out the transformation for the response, we find the lambda for it

```{r}
boxcox(m1)

```
We know that the most common Box-Cox Transformations are

|$\lambda$|Transformed Data|
|---------|-----------------|
|-2|$y^{-2}$|
|-1|$y^{-1}$|
|-.5|$1 \over \sqrt y$|
|0|ln(y)|
|.5|$\sqrt y$|
|1|y|
|2|$y^2$|

since our $\lambda$ is close to 0 we will do log transformations

We redo the model and look at the diagnostics plots again


```{r fig.height=5, fig.width=15, message=FALSE, warning=FALSE}
m2 <- lm("log(SalePrice)~.", data=df_cols_removed)
knitr::kable(data.frame(diagnostics(m2)))
```

We see that the plots and the diagnoistics are a lot better, but there seems to be some scope for improvement. 


#### Predictors transformation identification

Let us now identify the lambda transformations for the other columns we identified and using those variables as response, fit the model, but keep log(SalePrice) in the predictor with others


```{r}
m3 <- lm("LotArea~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for LotArea")
```

We should apply log transformation to LotArea since $\lambda$ is close to 0

```{r}
m3 <- lm("X1stFlrSF~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for X1stFlrSF")
```

There is no need to apply any transformation to X1stFlrSF

```{r}
m3 <- lm("GrLivArea~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for GrLivArea")
```

There is no need to apply any transformation to GrLivArea

```{r}
m3 <- lm("GarageArea~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for GarageArea")
```

We should apply log transformation to GarageArea since $\lambda$ is close to 0

```{r}
m3 <- lm("YearBuilt~.-SalePrice+log(SalePrice)", data = df_cols_removed)
bc <- boxcox(m3,xlab = "lambda for YearBuilt", lambda = seq(-200,400))
(best_lam <- bc$x[which(bc$y==max(bc$y))])
```

```{r}
m3 <- lm("YearRemodAdd~.-SalePrice+log(SalePrice)", data = df_cols_removed)
bc <- boxcox(m3,xlab = "lambda for YearRemodAdd", lambda = seq(-200,400))

(best_lam <- bc$x[which(bc$y==max(bc$y))])
```

YearBuilt and YearRemodAdd have very high $\lambda$ and using these transformations will make it very hard to explain the model. We will keep a note of these two, and experiment with them if necessary

We will not identify the Lambda for MasVnrArea, BsmtFinSF1, BsmtFinSF2, WoodDeckSF, OpenPorchSF, EnclosedPorch since those predictors constant 0's, and we cant run the boxcox function on them unless we handle the 0's. Since we have a lot of predictors, we will not message these 6 predictors.



## Model Identification



### Models - Simple, transformation, and using step {#id3}

Based on the above analysis we create the below models to start with

1) A simple additive model
2) A model with the above transformations but without the extreme transformations for YearBuilt and YearRemodAdd

```{r}
m_additive <- lm("SalePrice~.", data = df_cols_removed)
```

```{r}
frm <- create_formula(df_cols_removed, "log(SalePrice)", c("SalePrice", "LotArea", "GarageArea"), c("log(LotArea)", "log(GarageArea)"))

m_transform_1 <- lm(formula = frm, data = df_cols_removed)

```

Now we use step backwards with aic for the above to find better versions of these models that are smaller than them

```{r}
m_additive_step <- step(m_additive, trace = 0)
(frm <- formula(m_additive_step))
```

```{r}
m_transform_1_step <- step(m_transform_1, trace = 0)
(frm <- formula(m_transform_1_step))
```

We now compare the diagnostics of these models

```{r statcomp}
m_additive_result <- diagnostics(m_additive, plotit = FALSE)
m_additive_step_result <- diagnostics(m_additive_step, plotit = FALSE)

m_transform_1_result <- diagnostics(m_transform_1, plotit = FALSE)
m_transform_1_step_result <- diagnostics(m_transform_1_step, plotit = FALSE)

df_result <- rbind(m_additive = m_additive_result, 
                   m_additive_step = m_additive_step_result, 
                   m_transform_1 = m_transform_1_result, 
                   m_transform_1_step = m_transform_1_step_result)

knitr::kable(df_result)
```

Looking at the above table, we can clearly see that the additive model isnt yielding a good model. The RMSE is extremely high. Hence we will discard this model for now.




#### Anova test {#id4}

In order to confirm that the model generated using step (m_transform_1_step) is a better model than the m_transform_1 model, we will do an anova test

```{r}
anova(m_transform_1, m_transform_1_step)
```

Based on the anova test, we see that the smaller model is sufficient hence we move ahead with m_transform_1_step



#### Model selection

As we concluded above, we will use m_transform_1_step model going forward. Below is the formula of the model selected.

```{r}
formula(m_transform_1_step)
```




### Individual parameter significance test for selected model {#id5}

Looking at the diagnostics, our model can still do better. We will now look at the individual significant of the parameters of this model to see if we can eliminate any predictors

```{r}
a <- coef(summary(m_transform_1_step))[,"Pr(>|t|)"] 
names(a)
```


The above are all the coefficients of the model. We will use them to compare to the below filtered list of p-values > .1

We will use alpha = .1 
We now identify the individual columns that have p-value of greater than .1 and remove them from the dataset to create another model

```{r}
names(a[a>.01])
```



We will select all non-categorical variables that have $pvalue>.1$ and will will choose only the categorical predictors to remove that have all of the categories with $pvalue>.1$

* LandSlope - Intuitively we think this is important hence we will not remove it
* YearRemodAdd
* ExterCond
* KitchenAbvGr - we intuitively think this is important and will not remove it
* PavedDrive 
* EnclosedPorch
* SaleType - Intuitively we think this is important hence we will not remove it

Now we modify the formula of the model that is best so far, and remove the above identified predictors from it

```{r}
remove_cols3 <- c("YearRemodAdd","ExterCond", "PavedDrive", "EnclosedPorch")

f <- formula(m_transform_1_step)
predictor_list <- str_split(f, pattern = fixed(" + "))[[3]]
predictor_list <- predictor_list[!(predictor_list %in% remove_cols3)]
# replacing the \n that str_spit introduces after 500 characters
predictor_list <- str_replace(predictor_list, "\n    ", "")
# create the formula
(frm1 <- paste("log(SalePrice) ~ ", paste(predictor_list, collapse = ' + ')))

```

Now we use the above formula to create the model

```{r}
m_transform_1_step_sig_only <- lm(frm1, data = df_cols_removed)
```



#### Anova test

We do an anova test between the two models to make sure we have not discarded significant predictors

```{r}
anova(m_transform_1_step_sig_only, m_transform_1_step)
```

Based on the result of the anova test, we see that for our smaller model we fail to reject the Null Hypothesis, hence we move ahead with this model



#### Diagnostic comparison {#id6}

Now we will compare the diagnostics of the 2 models


```{r}
m_transform_1_step_result <- diagnostics(m_transform_1_step, plotit = FALSE)
m_transform_1_step_sig_only_result <- diagnostics(m_transform_1_step_sig_only, plotit = FALSE)

df_result <- rbind(m_transform_1_step=m_transform_1_step_result, m_transform_1_step_sig_only=m_transform_1_step_sig_only_result)

knitr::kable(df_result)
```

We see that our diagnostic statistics are about the same, with minor degradation of the diagnostics for the smaller model. 



#### Model selection

Since the diagnostics are almost similar, and we prefer smaller models, hence we will select the smaller model, m_transform_1_step_sig_only, as our better model



### Variance Inflation factor identification {#id7}

We look at variance inflation factors, and filter by only vifs that are >5

```{r}
faraway::vif(m_transform_1_step_sig_only)[faraway::vif(m_transform_1_step_sig_only)>5]
```

We notice that while there are high vif values, they are for categorical variables, and hence we choose to do nothing with this. There is YearBuilt, but it is not large enough for us to remove it, and individually we have already seen above, that it seems to have a significant relationship with the response. Hence we make no changes



### Influential points identification and handling {#id8}

We will now look at high influence points and investigate them

```{r}
influentials <- which(cooks.distance(m_transform_1_step_sig_only) > (4 / length(cooks.distance(m_transform_1_step_sig_only))))
length(influentials)
```

As an experiment we try and remove the influentials and see what impact this has on the diagnostics

```{r}
df_wo_influentials <- df_cols_removed[-influentials,]
df_only_influentials <- df_cols_removed[influentials,]

m_no_influentials <- lm(formula(m_transform_1_step_sig_only), data = df_wo_influentials)

```



#### Diagnostics Comparison

Now we compare the diagnostics data

```{r}

m_transform_1_step_sig_only_result <- diagnostics(m_transform_1_step_sig_only, plotit = FALSE)
m_no_influentials_result <- diagnostics(m_no_influentials, plotit = FALSE)

df_result <- rbind(m_transform_1_step_sig_only = m_transform_1_step_sig_only_result, 
                   m_no_influentials = m_no_influentials_result)

knitr::kable(df_result)
```

We see that our diagnostics have improved significantly, including BP test and saphiro test p-values.

We will have to sacrifice about 7% of the observations but the improvements in RMSE and Adjusted Rsquare are significant. 

So now we know that it is the influential points that are causing our model to have less than ideal diagnostics and hence we will discard the influential points and select the new model as our better model



## Selected model

Hence now our good model is m_no_influentials and the dataset is df_wo_influentials

```{r}
formula(m_no_influentials)
```


# Results



## Comparison of all models

We have already seen this in various places but we will now compare the diagnostics of all the models that we have seen to see how we have progressed


```{r}

df_result <- rbind(m_additive = m_additive_result,
                   m_additive_step = m_additive_step_result,
                   m_transform_1 = m_transform_1_result,
                   m_transform_1_step = m_transform_1_step_result,
                   m_transform_1_step_sig_only = m_transform_1_step_sig_only_result, 
                   m_no_influentials = m_no_influentials_result)

knitr::kable(df_result)
```



## Diagnostic plots of Selected Model

We also look at the diagnostics plot of our selected model

```{r fig.height=5, fig.width=15, message=FALSE, warning=FALSE}
diagnostics(m_no_influentials, testit = FALSE)

```

Our diagnostics plots look fairly good. Using an $\alpha = .001$, even our shapiro test and BP test validate the normality and equal variance assumptions. Hence we will select this model as our final model



# Discussion

We spent a lot of time inspecting and cleaning the data in \@ref(id1). The amount of effort isnt visible in this report, since it was primary inspecting the Hmisc::describe output, which was very detailed. The effort was worth it though since had we not spent the time, our diagnostics or our discovery process could have been fairly painful

We also used boxcox in section \@ref(id2) to identify response and predictor transformations. That was also valuable and as we saw in  \@ref(id3), it significantly improved the model.

What is not shown in the report is that we used bic to step back from our selected model in \@ref(id3), but the anova test for the selected model failed. For brevity of report, we only showed the aic step function, and as we can see in \@ref(id4), the anova test failed to reject the null Hypothesis for this smaller function

We spent a significant time looking at the individual significance test in \@ref(id5), and experimenting with the variables there. This analysis had a limited benefit, and resulted in only making our diagnostic parameters worse, as can be seen in \@ref(id6). We still picked the smaller model as a compromise of less predictors vs only minor diagnostic statistic worsening. We should investigate further if we had not removed the parameters here, would our predictions be significantly better

We looked at variance inflation factors in \@ref(id7), but chose not to remove any since our primary aim was better prediction, and the vifs werent extremely high

We then investigated the result of removing influential observations. As we can see in \@ref(id8) this had a significant improvement on the diagnostics, and it seemed like a good compromise

After looking at the diagnostics plot we come to the final conclusion that we have a good enough model


```{r}
knitr::kable(data.frame(diagnostics(m_no_influentials, plotit = FALSE)))
```


As we can see from the diagnostic statistics above, the model is useful since it has a fairly low RMSE and a very high adjusted $R^2$. Our AIC is also very low. We did look at the equal variance and the qq plot, and the plots look fairly good for such a large dataset. The shapiro test has a .001 p value and BP test has a .005 p-value. If we use an $\alpha = .001$, then we see that the normality and equal variance assumptions are met. For a data set this large, this is quite an accomplishment. 

Hence we can conclude that we have a fairly good model.


# Appendix

## Names of the team members

The team consisted of Praveen Purohit and Sunil Nandamuri


## Description of data

We spent a lot of time studying the data, but including it would clutter the report. We are showing it below

```{r}

Hmisc::describe(df1)
```


## Correlation Matrix

We looked at and analyzed the correlation matrix but only shared a small snippet of it. We share the complete correlation matrix below

```{r}
cor(df_only_numeric, use = "complete.obs")
```



