---
title: 'Final Project'
author: "STAT 420, Summer 2020, Mandalorians"
date: ''
output:
  bookdown::html_document2: 
    toc: true
  bookdown::pdf_document2: 
    toc: true
   
fig_caption: yes
urlcolor: cyan
---

\usepackage{subfig}

***


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE}

library("tibble")
library("readr")
library("Hmisc")
library("dplyr")
library("statsr")
library("GGally")
library("gridExtra")
library("MASS")
library("car")
library("lmtest")
library("stringr")
```

# Introduction

We have a housing dataset. We have pulled this dataset as part of a kaggle challenge. We have the train and test data as separate files. Since this is a challenge, we dont have the predicted values for the test dataset.

Our job is to predict house prices, given 80 predictors.

We can see the structure of the dataset below

```{r}
df1 <-read.csv("train.csv",as.is = FALSE )

str(df1)
```




Now we investigate the dataset to see what actions we need to take with the dataset before creating a model

```{r message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}
Hmisc::html(Hmisc::describe(df1))
```

Since the output is very long, we use echo=FALSE, but as a sample, we show the output below for the first 5 rows

```{r message=FALSE, warning=FALSE}
Hmisc::html(Hmisc::describe(df1[,1:5]), where=c('cwd', 'tmp'))
```


We have 80 predictors in the dataset. That tells us that we really arent short of predictors.

Based on the above dataset description (showing only first 5 for brevity) we draw the following conclusions


**Remove these columns**

* Remove Id column since it is not a predictor and is simply a running id
* Alley - has 1369 missing values and only 91 observations with values. 
* Street - Only 6 elements or 0.4% data elements have Street as Grvl, rest are all Pave. 6 is too few data elements to make a prediction 
* LotFrontage has 259 missing values. This is about 18% of total observations. If we were short of predictors we would not delete it. Since there a lot of predictors that convey space of the house, we will delete this column
* Utilities - Only 1 value has NoSeWa and the rest are all AllPub. Since only 1 element isnt enough for creating a model
* Condition2 - remove since 99% are Norm while the remaining 1% are distributed across 7 other values. This implies that the other 7 values dont have enough data to make a useful prediction
* RoofMatl - 98.2% is CompShg and the remaining 1.8% is spread across 7 values and the remaining 7 values dont have enough data to make a prediction
* X3SsnPorch - 98.4% is value 0, and the remaining 1.6% is spread across 19 types. Each of these has only 1 or 2 observtions hence not enough data for prediction
* PoolArea - 99.5% has value 0 and the remaining .5% is apread across 7 factors, implying not enough data in each type to make a model
* PoolQC - since only 7 observations have non missing values
* MiscFeature, MiscVal - since only 54 and 52 observations respectively  have values
* FirePlaceQu - has 690 values missing which is approximately 47% of total predictors. Since we are not short of number of predictors, and this one has half the values missing, we will delete this predictor
* Fence - 1179 values missing.


**Other Observations**

* MasVnrType and MasVnrArea - has 8 missing values
* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2 - each have 37 or 38 missing values
* Electrical = has 1 value missing
* GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond - has 81 values missing

Based on the above analysis we remove the ones that we want to remove and store the column names for the ones with missing values in a vector to be used later if necessary

```{r}
remove_cols <- c("Id", "Alley", "Street", "LotFrontage", "Utilities", "Condition2", "RoofMatl", "X3SsnPorch", "PoolArea", "PoolQC", "MiscFeature", "MiscVal", "FireplaceQu", "Fence")

col_with_missing_values <- c("MasVnrType", "MasVnrArea", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "Electrical", "GarageType", "GarageYrBlt", "GarageFinish", "GarageQual", "GarageCond")

keep_cols <- colnames(df1)[!(colnames(df1) %in% remove_cols)]

df_cols_removed <- subset(df1, select = as.vector(eval(keep_cols)))
```

Now, to identify columns that have high correlations. We define high correlation as great than 70% correlation.

In order to identify correlation we first create a dataframe that has only numeric predictors

```{r}
df_only_numeric <- df_cols_removed[,sapply(df_cols_removed, is.numeric)]
```

Now we identify the correlation in numeric predictors. We mark all correlations less than .75 as NA, so that we can easily identify highly corelated features

```{r}
cor_relation = cor(df_only_numeric, use = "complete.obs")
cor_relation[abs(cor_relation) < 0.75] <- NA
cor_relation
```


Based on the above correlation matrix, we make the below observations

**Remove the following predictors**

* YearBuilt and GarageYrBlt has .82 correlation coefficient. This makes sense since the Garage will be built in the same year as the house, with a few exceptions. We also reason that the price will be more tied to how old the house is. Since YearBuilt of house will be a more useful predictor, we will remove GarageYrBlt
* X1stFlrSF and TotalBsmtSF has a .82 correlation. This is obvious since the size of teh 1st floor will be very similar to size of basement. We can also reason that the price will be more correlated to the price of the 1st floor. Hence we will remove TotalBsmtSF
* GarageArea and GarageCars have a .83 correlation. Intutively we can see that both of these are related. Hence we remove GarageCars 
* GrLivArea and TotRmsAbvGrd has a correlation of .82. We see that number of Bedrooms and kitchens and batrooms are already captured separately. Hence we remove TotRmsAbvGrd

**Make a note of these**
* OverallQual has a .78 correlation with SalePrice. Given that, we should make sure OverallQual is part of the model



```{r}
remove_cols2 <- c("GarageYrBlt", "TotalBsmtSF", "GarageCars", "TotRmsAbvGrd")

keep_cols2 <- colnames(df_cols_removed)[!(colnames(df_cols_removed) %in% remove_cols2)]

df_cols_removed <- subset(df_cols_removed, select = as.vector(eval(keep_cols2)))

ncol(df_cols_removed)

```

We see that now we are left with 63 columns compared to the 81 that we started with.


We know that step wont work with missing values. Hence we will need to remove those. 

```{r}
df_cols_removed <- na.omit(df_cols_removed)
nrow(df_cols_removed)
```

We started with 1460 observations and 81 columns and after our initial cleanup we end up with 1338 observations and 63 columns

Now, before we begin modeling, we look at the pairs plots to see if any of the parameters are an obvious choice for transformations

For the purpose of being able to see the plots clearly, we do two things for the visual of the pair plot

1) We only look at numeric columns
2) We filter out the columns that have 25 or less discrete values, since they will probably not be candidates for transformations

We wont save this intermim dataset. It is only created to be able to see the distribution across many predictors clearly

```{r}
df_only_numeric <- df_cols_removed[,sapply(df_cols_removed, is.numeric)]
remove_for_plot <- c()
j=1
for(i in colnames(df_only_numeric)){
  k <- length(unique(df_only_numeric[,i])<20)
  if(k<20){
    remove_for_plot[j] <- i
    j <- j+1
  }
}

keep_for_plot <- colnames(df_only_numeric)[!(colnames(df_only_numeric) %in% remove_for_plot)]

df_cols_removed_for_plot <- subset(df_only_numeric, select = as.vector(eval(keep_for_plot)))

```



```{r fig.height=30, fig.width=35, message=FALSE, warning=FALSE, cache=TRUE}
#pairs(df_only_numeric)
ggpairs( df_cols_removed_for_plot, ggplot2::aes(color=I("navyblue")), title = "ggpairs plot to see correlation and distribution", lower = list(continuous = wrap("smooth")), axisLabels = "show", switch = "both")
```

We make the following observations from the plot

** Potential for transformations **

* LotArea
* MasVnrArea
* BsmtFinSF1
* BsmtFinSF2
* X1stFlrSF
* GrLivArea
* GarageArea
* WoodDeckSF
* OpenPorchSF
* EnclosedPorch
* YearBuilt
* YearRemodAdd 
* SalePrice

In order to validate that transformations are necessary we will start with a simple additive model and look at its diagnostics plots

```{r}
m1 <- lm("SalePrice~.", data=df_cols_removed)
plot(m1)
```


Based on the diagnostics plot we see that some kind of transformation for the response is necessary.

In order to figure out the transformation we find the lambda for it

```{r}
boxcox(m1)

```
We know that the most common Box-Cox Transformations are

|$\lambda$|Tranasformed Data|
|---------|-----------------|
|-2|$y^{-2}$|
|-1|$y^{-1}$|
|-.5|$1 \over \sqrt y$|
|0|ln(y)|
|.5|$\sqrt y$|
|1|y|
|2|$y^2$|

since our $\lambda$ is close to 0 we will do log transformations

We redo the model and look at the diagnostics plots again


```{r}
m2 <- lm("log(SalePrice)~.", data=df_cols_removed)
plot(m2)
```

We see that the plots are a lot better, but there seems to be some scope for improvement. 

Let us now identify the lamda transformations for the other columns we identified and using those variables as response, fit the model, but keep log(SalePrice) in the predictor with others


```{r}
m3 <- lm("LotArea~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for LotArea")
```

We should apply log transformation to LotArea since $\lambda$ is close to 0

```{r}
m3 <- lm("X1stFlrSF~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for X1stFlrSF")
```

There is no need to apply any transformation to X1stFlrSF

```{r}
m3 <- lm("GrLivArea~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for GrLivArea")
```

There is no need to apply any transformation to GrLivArea

```{r}
m3 <- lm("GarageArea~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for GarageArea")
```

We should apply log transformation to GarageArea since $\lambda$ is close to 0

```{r}
m3 <- lm("YearBuilt~.-SalePrice+log(SalePrice)", data = df_cols_removed)
bc <- boxcox(m3,xlab = "lambda for YearBuilt", lambda = seq(-200,400))
(best_lam <- bc$x[which(bc$y==max(bc$y))])
```

```{r}
m3 <- lm("YearRemodAdd~.-SalePrice+log(SalePrice)", data = df_cols_removed)
bc <- boxcox(m3,xlab = "lambda for YearRemodAdd", lambda = seq(-200,400))

(best_lam <- bc$x[which(bc$y==max(bc$y))])
```

YearBuilt and YearRemodAdd have very high $\lambda$ and using these transformations will make it very hard to explain the model. We will keep a note of these two, and experiment with them if necessary

We will not identify the Lambda for MasVnrArea, BsmtFinSF1, BsmtFinSF2, WoodDeckSF, OpenPorchSF, EnclosedPorch since those predictors contant 0's, and we cant run the boxcox funtion on them unless we handle the 0's. Since we have a lot of predictors, we will not message these 6 predictors.

 

Based on the above analysis we create 4 models to start with

1) A simple additive model
2) A model with the above transformations but without the extreme transformations for YearBuilt and YearRemodAdd

```{r}
create_formula <- function(data_set, response, cols_to_remove="", cols_to_add=""){
  
  predictor_list <- colnames(df_cols_removed)
  
  predictor_list <- predictor_list[!(predictor_list %in% cols_to_remove)]
  n <- length(predictor_list)

  for(i in 1:length(cols_to_add)){
    n <- n+1
    predictor_list[n] <- cols_to_add[i]
  }
  
  frm1 <- paste(response, " ~ ", paste(predictor_list, collapse = ' + '))
}
```

```{r}
num_predictors_in_formula <- function(model_formula){

  return(length(strsplit(as.character(model_formula)[3], fixed = TRUE, split = "+")[[1]]))
}

```

```{r cache=TRUE}
m_additive <- lm("SalePrice~.", data = df_cols_removed)
```

```{r}
frm <- create_formula(df_cols_removed, "log(SalePrice)", c("SalePrice", "LotArea", "GarageArea"), c("log(LotArea)", "log(GarageArea)"))

m_transform_1 <- lm(formula = frm, data = df_cols_removed)

```

```{r}
frm <- create_formula(df_cols_removed, "log(SalePrice)", c("SalePrice", "LotArea", "GarageArea", "YearBuilt", "YearRemodAdd"), c("log(LotArea)", "log(GarageArea)", "YearBuilt^35", "YearRemodAdd^55"))

m_transform_2 <- lm(formula = frm, data = df_cols_removed)

```

Now we use step backwards with aic for all 3 models above to find 3 good models which are smaller than the above

```{r cache=TRUE}
m_additive_step <- step(m_additive, trace = 0)
(frm <- formula(m_additive_step))
```

```{r cache=TRUE}
m_transform_1_step <- step(m_transform_1, trace = 0)
(frm <- formula(m_transform_1_step))
```

By using AIC to step backwards we are not penalizing the models for the number of predictors. Since the models are still huge, we will run step BIC backwards on the results of the step AIC with the hope of getting smaller models

```{r cache=TRUE}
m_additive_step_bic <- step(m_additive_step, trace = 0, k=log(nrow(df_cols_removed)))
(frm <- formula(m_additive_step_bic))
```

```{r cache=TRUE}
m_transform_1_step_bic <- step(m_transform_1_step, trace = 0, k=log(nrow(df_cols_removed)))
(frm <- formula(m_transform_1_step_bic))
num_predictors_in_formula(frm)
```


```{r}
diagnostics <- function(model = fit_1, pcol = 'dodgerblue', lcol = 'red', alpha = .05, plotit = TRUE, testit = TRUE){
  
  if(plotit == TRUE){
    g1 <- ggplot(data = model, aes(sample=.resid)) + 
      stat_qq(color=I(pcol)) + stat_qq_line(color = I(lcol)) +
      ggtitle("Normal QQ Plot") +  theme_light() 
    
    g2 <- ggplot(data = model, aes(x = fitted(model), y = resid(model))) +
      geom_point(color=I(pcol)) + geom_hline(yintercept=0, color = I(lcol)) +
      xlab("Fitted") + ylab("Residuals") + ggtitle("Residuals vs Fitted Plot") + theme_light() 
    
    grid.arrange(g1, g2, ncol=2)
  }
  
  if(testit == TRUE){
    shapiro_Normalcy_test_result <- shapiro.test(resid(model))$"p.value"
    
    bptest_Const_Variance_test_result <-  bptest(model)$"p.value"[[1]]
    
    rmse <- round(sqrt(mean(resid(model) ^ 2)), 4)
    loocv_cross_validated_rmse <- round(sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2)), 4)
    aic <- extractAIC(model)[2]
    num_predictors <- num_predictors_in_formula(formula(model))
    
    l1 <- list(num_predictors=num_predictors, shapiro_Normalcy_test_pvalue=shapiro_Normalcy_test_result, bptest_Const_Variance_test_pvalue=bptest_Const_Variance_test_result, RMSE=rmse, loocv_cross_validated_rmse=loocv_cross_validated_rmse, AdjustedR2=summary(model)$"adj.r.squared", AIC=aic)
    
    return(l1)
  }
}
```


```{r fig.height=7, fig.width=15}
m_additive_step_result <- diagnostics(m_additive_step, plotit = FALSE)
m_additive_step_bic_result <- diagnostics(m_additive_step_bic, plotit = FALSE)

m_transform_1_step_result <- diagnostics(m_transform_1_step, plotit = FALSE)
m_transform_1_step_bic_result <- diagnostics(m_transform_1_step_bic, plotit = FALSE)

df_result <- rbind(m_additive_step=m_additive_step_result, m_additive_step_bic=m_additive_step_bic_result, m_transform_1_step=m_transform_1_step_result, m_transform_1_step_bic=m_transform_1_step_bic_result)

```
```{r}
knitr::kable(df_result)
```

Looking at the above table, we can clearly see that the additive model isnt yielding a good model after stepping back using AIC and BIC. The RMSE is extremely high. Hence we will discard this model for now.

This leaves us with 2 models to evaluate

```{r}
formula(m_transform_1_step)

formula(m_transform_1_step_bic)
```

We can see that m_transform_1_step_bic is a subset of m_transform_1_step. We will look at the annova test to see which of these two is better

```{r}
anova(m_transform_1_step_bic, m_transform_1_step)
```

Looking at the p-value we can say that the larger model has atleast some of the parameters that have a significant relationship with the predictor. Hence we will continue to use the larger model

Let us look at the diagnostics for this model

```{r fig.width=15}
diagnostics(m_transform_1_step)

```

Looking at the diagnostics, our model can still do better. We will now look at the individual significant of the parameters of this model to see if we can eliminate any predictors

```{r}
a <- coef(summary(m_transform_1_step))[,"Pr(>|t|)"] 
names(a)
```

We will be conservative and use $\alpha = .1  $. We now identify the individual columns that have p-value of greater than .1 and remove them from the dataset to create another model

```{r}
names(a[a>.1])
```

We will select all non-categorical variables that have $pvalue>.1$ and will will choose only the categorical predictors to remove that have all of the categories with $pvalue>.1$

* LandSlope - Intutively we think this is important hence we will not remove it
* YearRemodAdd
* ExterCond
* BsmtFinType
* PavedDrive 
* EnclosedPorch
* SaleType - Intutively we think this is important hence we will not remove it

Now we modify the formula of the model that is best so far, m_transform_1, and remove the above 5 predictors from it

```{r}
remove_cols3 <- c("YearRemodAdd","ExterCond","PavedDrive", "EnclosedPorch", "BsmtFinType")

f <- formula(m_transform_1_step)
predictor_list <- str_split(f, pattern = fixed(" + "))[[3]]
predictor_list <- predictor_list[!(predictor_list %in% remove_cols3)]
# replacing the \n that str_spit introduces after 500 characters
predictor_list <- str_replace(predictor_list, "\n    ", "")
# create the formula
(frm1 <- paste("log(SalePrice) ~ ", paste(predictor_list, collapse = ' + ')))

```

Now we use the above formula to create the model and look at its diagnostics

```{r}
m_transform_1_step_sig_only <- lm(frm1, data = df_cols_removed)
```

We do an anova test between the two models to make sure we have not discarded significant predictors

```{r}
anova(m_transform_1_step_sig_only, m_transform_1_step)
```

Based on the result of the anova test, we see that for our smaller model we fail to reject the Null Hypothesis, hence we move ahead with this model

Now we will compare the diagnostics of the 2 models


```{r fig.height=7, fig.width=15}
m_transform_1_step_result <- diagnostics(m_transform_1_step, plotit = FALSE)
m_transform_1_step_sig_only_result <- diagnostics(m_transform_1_step_sig_only, plotit = FALSE)

df_result <- rbind(m_transform_1_step=m_transform_1_step_result, m_transform_1_step_sig_only=m_transform_1_step_sig_only_result)

knitr::kable(df_result)
```

We see that our diagnostic staticstics are about the same. Hence we will select the smaller model, m_transform_1_step_sig_only, as our better model

we will now look at the diagnostics plots for this model

```{r}
plot(m_transform_1_step_sig_only)
```

Our diagnostics plots seem to be looking good. 

We look at variance inflation factors, and filter by only vifs that are >5

```{r}
vif(m_transform_1_step_sig_only)[vif(m_transform_1_step_sig_only)>5]
```

We notice that while there are high vif values, they are for categorical variables, and hence we choose to do nothing with this


We will now look at high influence points and investigate them

```{r}
(influentials <- which(cooks.distance(m_transform_1_step_sig_only) > (4 / length(cooks.distance(m_transform_1_step_sig_only)))))
```

As an experiment we try and remove the influentials and see what impact this has on the diagnostics

```{r}
df_wo_influentials <- df_cols_removed[-influentials,]
df_only_influentials <- df_cols_removed[influentials,]

m_no_influentials <- lm(formula(m_transform_1_step_sig_only), data = df_wo_influentials)


```



```{r}

m_transform_1_step_sig_only_result <- diagnostics(m_transform_1_step_sig_only, plotit = FALSE)
m_no_influentials_result <- diagnostics(m_no_influentials, plotit = FALSE)

df_result <- rbind(m_transform_1_step_sig_only=m_transform_1_step_sig_only_result , m_no_influentials=m_no_influentials_result)

knitr::kable(df_result)
```


We see that our diagnostics have improved significantly. So now we know that it is the influential points that are causing our model 

Now we need to figure out if removing the influentials is the right thing to do

```{r}
df_only_influentials

```

Looking at the dataset that just has the influential points, we decide to keep them, since we dont see a real justification for removing them.

We had noted at the start that OverallCond had .78 correlation with SalePrice. We will now use just log(SalePrice)~OverallCond as the starting model and step forward with the scope being the current best model

```{r}
m_small <- lm("log(SalePrice)~OverallCond", data = df_cols_removed)

m_small_step <- step(m_small, scope = formula(m_transform_1_step_sig_only), direction = "forward", trace = 0)

```

We will now look at diagnostics data with these three models

```{r}

m_transform_1_step_sig_only_result <- diagnostics(m_transform_1_step_sig_only, plotit = FALSE)
m_small_step_result <- diagnostics(m_small_step, plotit = FALSE)

df_result <- rbind(m_transform_1_step=m_transform_1_step_result, m_small_step=m_small_step_result)

knitr::kable(df_result)
```
Most of the diagnostics are the same, but the number of predictors is less for the m_small_step model.

Hence we will use this as our final model.

We look at the diagnostics plot finally
```{r}
plot(m_small_step)

```


Our diagnostics plots look fairly ok. We know that our shapiro test and BP test are rejecting the null hypothesis, but since the graphs looks pretty good, and we know that these tests are quick to reject for large datasets, we will move ahead with this model




