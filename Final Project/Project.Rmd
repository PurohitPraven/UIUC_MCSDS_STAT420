---
title: "Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}

library("tibble")
library("readr")
library("Hmisc")
library("dplyr")
library("statsr")
library("GGally")
library("gridExtra")
library("MASS")
library("car")

library(stringr)
```


We first read the training dataset and look at its structure

```{r}
df1 <-read.csv("train.csv",as.is = FALSE )

str(df1)
```

Now we investigate the dataset to see what actions we need to take with the dataset before creating a model

```{r message=FALSE, warning=FALSE, echo=FALSE, include=FALSE}
Hmisc::html(Hmisc::describe(df1))
```

Since the output is very long, we use echo=FALSE, but as a sample, we show the output below for the first 5 rows

```{r message=FALSE, warning=FALSE}
Hmisc::html(Hmisc::describe(df1[,1:5]), where=c('cwd', 'tmp'))
```


We have 80 predictors in the dataset. That tells us that we really arent short of predictors.

Based on the above dataset description (showing only first 5 for brevity) we draw the following conclusions


**Remove these columns**

* Remove Id column since it is not a predictor and is simply a running id
* Alley - has 1369 missing values and only 91 observations with values. 
* Street - Only 6 elements or 0.4% data elements have Street as Grvl, rest are all Pave. 6 is too few data elements to make a prediction 
* LotFrontage has 259 missing values. This is about 18% of total observations. If we were short of predictors we would not delete it. Since there a lot of predictors that convey space of the house, we will delete this column
* Utilities - Only 1 value has NoSeWa and the rest are all AllPub. Since only 1 element isnt enough for creating a model
* Condition2 - remove since 99% are Norm while the remaining 1% are distributed across 7 other values. This implies that the other 7 values dont have enough data to make a useful prediction
* RoofMatl - 98.2% is CompShg and the remaining 1.8% is spread across 7 values and the remaining 7 values dont have enough data to make a prediction
* X3SsnPorch - 98.4% is value 0, and the remaining 1.6% is spread across 19 types. Each of these has only 1 or 2 observtions hence not enough data for prediction
* PoolArea - 99.5% has value 0 and the remaining .5% is apread across 7 factors, implying not enough data in each type to make a model
* PoolQC - since only 7 observations have non missing values
* MiscFeature, MiscVal - since only 54 and 52 observations respectively  have values
* FirePlaceQu - has 690 values missing which is approximately 47% of total predictors. Since we are not short of number of predictors, and this one has half the values missing, we will delete this predictor
* Fence - 1179 values missing.


**Other Observations**

* MasVnrType and MasVnrArea - has 8 missing values
* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2 - each have 37 or 38 missing values
* Electrical = has 1 value missing
* GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond - has 81 values missing

Based on the above analysis we remove the ones that we want to remove and store the column names for the ones with missing values in a vector to be used later if necessary

```{r}
remove_cols <- c("Id", "Alley", "Street", "LotFrontage", "Utilities", "Condition2", "RoofMatl", "X3SsnPorch", "PoolArea", "PoolQC", "MiscFeature", "MiscVal", "FireplaceQu", "Fence")

col_with_missing_values <- c("MasVnrType", "MasVnrArea", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "Electrical", "GarageType", "GarageYrBlt", "GarageFinish", "GarageQual", "GarageCond")

keep_cols <- colnames(df1)[!(colnames(df1) %in% remove_cols)]

df_cols_removed <- subset(df1, select = as.vector(eval(keep_cols)))
```

Now, to identify columns that have high correlations. We define high correlation as great than 70% correlation.

In order to identify correlation we first create a dataframe that has only numeric predictors

```{r}
df_only_numeric <- df_cols_removed[,sapply(df_cols_removed, is.numeric)]
```

Now we identify the correlation in numeric predictors. We mark all correlations less than .75 as NA, so that we can easily identify highly corelated features

```{r}
cor_relation = cor(df_only_numeric, use = "complete.obs")
cor_relation[abs(cor_relation) < 0.75] <- NA
cor_relation
```


Based on the above correlation matrix, we make the below observations

**Remove the following predictors**

* YearBuilt and GarageYrBlt has .82 correlation coefficient. This makes sense since the Garage will be built in the same year as the house, with a few exceptions. We also reason that the price will be more tied to how old the house is. Since YearBuilt of house will be a more useful predictor, we will remove GarageYrBlt
* X1stFlrSF and TotalBsmtSF has a .82 correlation. This is obvious since the size of teh 1st floor will be very similar to size of basement. We can also reason that the price will be more correlated to the price of the 1st floor. Hence we will remove TotalBsmtSF
* GarageArea and GarageCars have a .83 correlation. Intutively we can see that both of these are related. Hence we remove GarageCars 
* GrLivArea and TotRmsAbvGrd has a correlation of .82. We see that number of Bedrooms and kitchens and batrooms are already captured separately. Hence we remove TotRmsAbvGrd

**Make a note of these**
* OverallQual has a .78 correlation with SalePrice. Given that, we should make sure OverallQual is part of the model



```{r}
remove_cols2 <- c("GarageYrBlt", "TotalBsmtSF", "GarageCars", "TotRmsAbvGrd")

keep_cols2 <- colnames(df_cols_removed)[!(colnames(df_cols_removed) %in% remove_cols2)]

df_cols_removed <- subset(df_cols_removed, select = as.vector(eval(keep_cols2)))

ncol(df_cols_removed)

```

We see that now we are left with 63 columns compared to the 81 that we started with.


We know that step wont work with missing values. Hence we will need to remove those. 

```{r}
df_cols_removed <- na.omit(df_cols_removed)
nrow(df_cols_removed)
```

We started with 1460 observations and 81 columns and after our initial cleanup we end up with 1338 observations and 63 columns

Now, before we begin modeling, we look at the pairs plots to see if any of the parameters are an obvious choice for transformations

For the purpose of being able to see the plots clearly, we do two things for the visual of the pair plot

1) We only look at numeric columns
2) We filter out the columns that have 25 or less discrete values, since they will probably not be candidates for transformations

We wont save this intermim dataset. It is only created to be able to see the distribution across many predictors clearly

```{r}
df_only_numeric <- df_cols_removed[,sapply(df_cols_removed, is.numeric)]
remove_for_plot <- c()
j=1
for(i in colnames(df_only_numeric)){
  k <- length(unique(df_only_numeric[,i])<20)
  if(k<20){
    remove_for_plot[j] <- i
    j <- j+1
  }
}

keep_for_plot <- colnames(df_only_numeric)[!(colnames(df_only_numeric) %in% remove_for_plot)]

df_cols_removed_for_plot <- subset(df_only_numeric, select = as.vector(eval(keep_for_plot)))

```



```{r fig.height=30, fig.width=35, message=FALSE, warning=FALSE, cache=TRUE}
#pairs(df_only_numeric)
ggpairs( df_cols_removed_for_plot, ggplot2::aes(color=I("navyblue")), title = "ggpairs plot to see correlation and distribution", lower = list(continuous = wrap("smooth")), axisLabels = "show", switch = "both")
```

We make the following observations from the plot

** Potential for transformations **

* LotArea
* MasVnrArea
* BsmtFinSF1
* BsmtFinSF2
* X1stFlrSF
* GrLivArea
* GarageArea
* WoodDeckSF
* OpenPorchSF
* EnclosedPorch
* YearBuilt
* YearRemodAdd 
* SalePrice

In order to validate that transformations are necessary we will start with a simple additive model and look at its diagnostics plots

```{r}
m1 <- lm("SalePrice~.", data=df_cols_removed)
plot(m1)
```


Based on the diagnostics plot we see that some kind of transformation for the response is necessary.

In order to figure out the transformation we find the lambda for it

```{r}
boxcox(m1)

```
We know that the most common Box-Cox Transformations are

|$\lambda$|Tranasformed Data|
|---------|-----------------|
|-2|$y^{-2}$|
|-1|$y^{-1}$|
|-.5|$1 \over \sqrt y$|
|0|ln(y)|
|.5|$\sqrt y$|
|1|y|
|2|$y^2$|

since our $\lambda$ is close to 0 we will do log transformations

We redo the model and look at the diagnostics plots again


```{r}
m2 <- lm("log(SalePrice)~.", data=df_cols_removed)
plot(m2)
```

We see that the plots are a lot better, but there seems to be some scope for improvement. 

Let us now identify the lamda transformations for the other columns we identified and using those variables as response, fit the model, but keep log(SalePrice) in the predictor with others


```{r}
m3 <- lm("LotArea~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for LotArea")
```

We should apply log transformation to LotArea since $\lambda$ is close to 0

```{r}
m3 <- lm("X1stFlrSF~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for X1stFlrSF")
```

There is no need to apply any transformation to X1stFlrSF

```{r}
m3 <- lm("GrLivArea~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for GrLivArea")
```

There is no need to apply any transformation to GrLivArea

```{r}
m3 <- lm("GarageArea~.-SalePrice+log(SalePrice)", data = df_cols_removed)
boxcox(m3,xlab = "lambda for GarageArea")
```

We should apply log transformation to GarageArea since $\lambda$ is close to 0

```{r}
m3 <- lm("YearBuilt~.-SalePrice+log(SalePrice)", data = df_cols_removed)
bc <- boxcox(m3,xlab = "lambda for YearBuilt", lambda = seq(-200,400))
(best_lam <- bc$x[which(bc$y==max(bc$y))])
```

```{r}
m3 <- lm("YearRemodAdd~.-SalePrice+log(SalePrice)", data = df_cols_removed)
bc <- boxcox(m3,xlab = "lambda for YearRemodAdd", lambda = seq(-200,400))

(best_lam <- bc$x[which(bc$y==max(bc$y))])
```

YearBuilt and YearRemodAdd have very high $\lambda$ and using these transformations will make it very hard to explain the model. We will keep a note of these two to experiment with, if necessary

We will not identify the Lambda for MasVnrArea, BsmtFinSF1, BsmtFinSF2, WoodDeckSF, OpenPorchSF, EnclosedPorch since those predictors contant 0's, and we cant run the boxcox funtion on them unless we handle the 0's. Since we have a lot of predictors, we will not message these 6 predictors.

 

Based on the above analysis we create 4 models to start with

1) A simple additive model
2) A model with the above transformations but without the extreme transformations for YearBuilt and YearRemodAdd
2) A model with the above transformations including the extreme transformations for YearBuilt and YearRemodAdd

```{r cache=TRUE}
m_additive <- lm("SalePrice~.", data = df_cols_removed)
```

```{r}
predictor_list <- colnames(df_cols_removed)
predictor_list <- predictor_list[!(predictor_list %in% c("LotArea", "GarageArea", "SalePrice"))]
n <- length(predictor_list)
n <- n+1
predictor_list[n] <- "log(LotArea)"
n <- n+1
predictor_list[n] <- "log(GarageArea)"
frm1 <- paste("log(SalePrice) ~ ", paste(predictor_list, collapse = ' + '))
m_transform_1 <- lm(formula = frm1, data = df_cols_removed)

```

```{r}
predictor_list <- colnames(df_cols_removed)
predictor_list <- predictor_list[!(predictor_list %in% c("LotArea", "GarageArea", "SalePrice", "YearBuilt", "YearRemodAdd"))]
n <- length(predictor_list)
n <- n+1
predictor_list[n] <- "log(LotArea)"
n <- n+1
predictor_list[n] <- "log(GarageArea)"
n <- n+1
predictor_list[n] <- "YearBuilt^35"
n <- n+1
predictor_list[n] <- "YearRemodAdd^55"

frm2 <- paste("log(SalePrice) ~ ", paste(predictor_list, collapse = ' + '))
m_transform_2 <- lm(formula = frm2, data = df_cols_removed)

```


Now we use step backwards with aic for all 3 models above to find 3 good models which are smaller than the above

```{r cache=TRUE}
m_additive_step <- step(m_additive, trace = 0)
```

```{r}
(frm1 <- formula(m_additive_step))
length((strsplit(frm1,"+", fixed = TRUE))[[1]])
paste("number of predictors = ", )

```

```{r cache=TRUE}
m_transform_1_step <- step(m_transform_1, trace = 0)
```

```{r}
formula(m_transform_1_step)
```

```{r cache=TRUE}
m_transform_2_step <- step(m_transform_2, trace = 0)
```

```{r}
formula(m_transform_2_step)
```

Since the models are still huge, we will run step BIC on the results of the step AIC

```{r cache=TRUE}
m_additive_step_bic <- step(m_additive_step, trace = 0)
```

```{r}
formula(m_additive_step)
```

```{r cache=TRUE}
m_transform_1_step_bic <- step(m_transform_1_step, trace = 0)
```

```{r}
formula(m_transform_1_step_bic)
```


```{r cache=TRUE}
m_transform_2_step_bic <- step(m_transform_2_step, trace = 0)
```

```{r}
formula(m_transform_2_step_bic)
```


```{r}
diagnostics <- function(model = fit_1, pcol = 'dodgerblue', lcol = 'red', alpha = .05, plotit = TRUE, testit = TRUE){
  
  if(plotit == TRUE){
    g1 <- ggplot(data = model, aes(sample=.resid)) + 
      stat_qq(color=I(pcol)) + stat_qq_line(color = I(lcol)) +
      ggtitle("Normal QQ Plot") +  theme_light() 
    
    g2 <- ggplot(data = model, aes(x = fitted(model), y = resid(model))) +
      geom_point(color=I(pcol)) + geom_hline(yintercept=0, color = I(lcol)) +
      xlab("Fitted") + ylab("Residuals") + ggtitle("Residuals vs Fitted Plot") + theme_light() 
    
    grid.arrange(g1, g2, ncol=2)
  }
  
  if(testit == TRUE){
    shapiro_Normalcy_test_result <- shapiro.test(resid(model))$"p.value"
    
    bptest(model)
    bptest_Const_Variance_test_result <-  bptest(model)$"p.value"
    
    model <- m_additive
    rmse <- sqrt(mean(resid(model) ^ 2))
    loocv_cross_validated_rmse <- sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
    aic <- extractAIC(model)[2]
    
    l1 <- list(shapiro_Normalcy_test_result=shapiro_Normalcy_test_result, bptest_Const_Variance_test_result=bptest_Const_Variance_test_result, RMSE=rmse, loocv_cross_validated_rmse=loocv_cross_validated_rmse, AdjustedR2=summary(model)$"adj.r.squared", AIC=aic)
    
    return(l1)
  }
}
```


```{r fig.height=7, fig.width=15}
m_additive_step_result <- diagnostics(m_additive_step)
m_transform_1_step_result <- diagnostics(m_transform_1_step)
m_transform_2_step_result <- diagnostics(m_transform_2_step)

df_result <- rbind(m_additive_step=m_additive_step_result, m_transform_1_step=m_transform_1_step_result, m_transform_2_step=m_transform_2_step_result)

```
```{r}
knitr::kable(df_result)
```



