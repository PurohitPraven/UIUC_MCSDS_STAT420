---
title: "Week 8 - Homework"
author: "STAT 420, Summer 2020, purohit4@illinois.edu"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
library(ggplot2)
library(gridExtra)
```

## Exercise 1 (Writing Functions)

**(a)** Write a function named `diagnostics` that takes as input the arguments:

- `model`, an object of class `lm()`, that is a model fit via `lm()`
- `pcol`, for controlling point colors in plots, with a default value of `grey`
- `lcol`, for controlling line colors in plots, with a default value of `dodgerblue`
- `alpha`, the significance level of any test that will be performed inside the function, with a default value of `0.05`
- `plotit`, a logical value for controlling display of plots with default value `TRUE`
- `testit`, a logical value for controlling outputting the results of tests with default value `TRUE`

The function should output:

- A list with two elements when `testit` is `TRUE`:
    - `p_val`, the p-value for the Shapiro-Wilk test for assessing normality
    - `decision`, the decision made when performing the Shapiro-Wilk test using the `alpha` value input to the function. "Reject" if the null hypothesis is rejected, otherwise "Fail to Reject."
- Two plots, side-by-side, when `plotit` is `TRUE`:
    - A fitted versus residuals plot that adds a horizontal line at $y = 0$, and labels the $x$-axis "Fitted" and the $y$-axis "Residuals." The points and line should be colored according to the input arguments. Give the plot a title. 
    - A Normal Q-Q plot of the residuals that adds the appropriate line using `qqline()`. The points and line should be colored according to the input arguments. Be sure the plot has a title. 

Consider using this function to help with the remainder of the assignment as well.

```{r fig.width=15}
diagnostics <- function(model = fit_1, pcol = 'grey', lcol = 'dodgerblue', alpha = .05, plotit = TRUE, testit = TRUE){
  
  if(plotit == TRUE){
    g1 <- ggplot(data = model, aes(sample=.resid)) + 
      stat_qq(color=I(pcol)) + stat_qq_line(color = I(lcol)) +
      ggtitle("Normal QQ Plot") +  theme_light() 
    
    g2 <- ggplot(data = model, aes(x = fitted(model), y = resid(model))) +
      geom_point(color=I(pcol)) + geom_hline(yintercept=0, color = I(lcol)) +
      xlab("Fitted") + ylab("Residuals") + ggtitle("Residuals vs Fitted Plot") + theme_light() 
    
    grid.arrange(g1, g2, ncol=2)
  }
  
  if(testit == TRUE){
    pval <- shapiro.test(resid(model))$"p.value"
    decision <- "Fail to Reject."
    if(pval < alpha){
      decision <- "Reject"
    }
    return(list(p_val=pval, decision=decision))
  }
}
```

<br><br>

**(b)** Run the following code.

```{r}
set.seed(40)

data_1 = data.frame(x = runif(n = 30, min = 0, max = 10),
                    y = rep(x = 0, times = 30))
data_1$y = with(data_1, 2 + 1 * x + rexp(n = 30))
fit_1 = lm(y ~ x, data = data_1)

data_2 = data.frame(x = runif(n = 20, min = 0, max = 10),
                    y = rep(x = 0, times = 20))
data_2$y = with(data_2, 5 + 2 * x + rnorm(n = 20))
fit_2 = lm(y ~ x, data = data_2)

data_3 = data.frame(x = runif(n = 40, min = 0, max = 10),
                    y = rep(x = 0, times = 40))
data_3$y = with(data_3, 2 + 1 * x + rnorm(n = 40, sd = x))
fit_3 = lm(y ~ x, data = data_3)
```


```{r fig.width=15, hold = "FALSE"}
diagnostics(fit_1, plotit = FALSE)$p_val
diagnostics(fit_2, plotit = FALSE)$decision
diagnostics(fit_1, testit = FALSE, pcol = "black", lcol = "black")
diagnostics(fit_2, testit = FALSE, pcol = "grey", lcol = "green")
diagnostics(fit_3)
```


***

## Exercise 2 (Prostate Cancer Data)

For this exercise, we will use the `prostate` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?prostate` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
library(lmtest)
```

```{r}
# Add BP_Test to above function and add better description to result
diagnostics2 <- function(model = fit_1, pcol = 'grey', lcol = 'dodgerblue', 
                         alpha = .05, plotit = TRUE, testit = TRUE)
{
  if(testit == TRUE){
    shapiro_Normalcy_test_result <- diagnostics(model, pcol, lcol, alpha, plotit, testit)
  
    bptest(model)
    pval <-  bptest(model)$"p.value"
    decision <- "Fail to Reject."
    if(pval < alpha){
      decision <- "Reject"
    }
    bptest_Const_Variance_test_result <- list(p_val=pval, decision=decision)
    df1 <- rbind(shapiro_Normalcy_test_result, bptest_Const_Variance_test_result)
    
    return(df1)
  }
  else{
    diagnostics(model, pcol, lcol, alpha, plotit, testit)
  }
}
```

<br><br>

**(a)** Fit an additive multiple regression model with `lpsa` as the response and the remaining variables in the `prostate` dataset as predictors. Report the $R^2$ value for this model.
```{r}
m1 <- lm(formula = lpsa ~ . , data = prostate)
summary(m1)$"r.squared"
```

<br><br>

**(b)** Check the constant variance assumption for this model. Do you feel it has been violated? Justify your answer.

```{r fig.width=15}
diagnostics2(m1)
```

Based on the above Residuals vs Fitted plot, and the BP_test p-value and decision we feel that the constant variance assumption has not been violated. If we look at the graph, we see that the variance at any value of x seems to be approximately the same even though the number of points in the middle are more, the variance seems to be the same

<br><br>

**(c)** Check the normality assumption for this model. Do you feel it has been violated? Justify your answer.
```{r}
diagnostics2(m1, plotit = FALSE)
```

Based on the QQ plot we see in the answer to part b or question 2 above, we observe that most of the points fall on the line. We do observe that it has fat tails. Hence it is hard to say that the normality condition is met or violated by simply looking at the graph. To be sure we look at the BP_test results in the above table. Based on the BP_test we Fail to reject the Null Hypothesis and say that normality assumptions are not violated.

<br><br>

**(d)** Check for any high leverage observations. Report any observations you determine to have high leverage.

```{r}
prostate[names(hatvalues(m1)[(hatvalues(m1)>2*mean(hatvalues(m1)))]),]
```
The above observations have high leverage


<br><br>

**(e)** Check for any influential observations. Report any observations you determine to be influential.

```{r}
(prostate_influentials <- prostate[names(which(cooks.distance(m1)>4/nrow(prostate))),])
```

The above observations seem to have high cook's distance (greater than 4/sample_size) and hence can be considered to have high influence


<br><br>

**(f)** Refit the additive multiple regression model without any points you identified as influential. Compare the coefficients of this fitted model to the previously fitted model.
```{r}
prostrate_wo_influentials <- prostate[(names(which(cooks.distance(m1)<=4/nrow(prostate)))),]
m2 <- lm(formula = lpsa ~ . , data = prostrate_wo_influentials)

df1 <- as.vector((coef(m1)-coef(m2))/coef(m1))

df2 <- rbind(coef(m1), coef(m2), df1)
rownames(df2) <- c("original prostate dataset", "prostate dataset without infludential points", "%age difference")
knitr::kable(df2)
```

We see that Intercept changes by in sign and has the largest percentage change. gleason also has a significant percentage change. next highest change is seen by pgg45 and lcp. Next in line are lbph and lweight. Others see minimal change  


<br><br>

**(g)** Create a data frame that stores the observations that were "removed" because they were influential. Use the two models you have fit to make predictions with these observations. Comment on the difference between these two sets of predictions.

```{r}
df1 <- predict(m1, newdata = prostate_influentials)
df2 <- predict(m2, newdata = prostate_influentials)
df3 <- as.vector((df1-df2)/df1)
df4 <- rbind(df1, df2, df3)
row.names(df4) <- c("prediction of lpsa with influential points included", "prediction of lpsa with no influential points", "%age difference")

knitr::kable(df4)

```


We see that the percentage change is very small. Predicted values show minor change but no drastic changes are observed. This would imply that we would do nothing with the influential points and will keep them in the dataset 

***

## Exercise 3 (Why Bother?)

**Why** do we care about violations of assumptions? One key reason is that the distributions of the parameter estimators that we have used are all reliant on these assumptions. When the assumptions are violated, the distributional results are not correct, so our tests are garbage. **Garbage In, Garbage Out!**

Consider the following setup that we will use for the remainder of the exercise. We choose a sample size of 50.

```{r}
n = 50
set.seed(420)
x_1 = runif(n, 0, 5)
x_2 = runif(n, -2, 2)
```

Consider the model,

\[
Y = 4 + 1 x_1 + 0 x_2 + \epsilon.
\]

That is,

- $\beta_0$ = 4
- $\beta_1$ = 1
- $\beta_2$ = 0

We now simulate `y_1` in a manner that does **not** violate any assumptions, which we will verify. In this case $\epsilon \sim N(0, 1).$

```{r}
set.seed(83)
library(lmtest)
y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = 1)
fit_1 = lm(y_1 ~ x_1 + x_2)
bptest(fit_1)
```

Then, we simulate `y_2` in a manner that **does** violate assumptions, which we again verify. In this case $\epsilon \sim N(0, \sigma = |x_2|).$

```{r}
set.seed(83)
y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = abs(x_2))
fit_2 = lm(y_2 ~ x_1 + x_2)
bptest(fit_2)
```

<br><br>

**(a)** Use the following code after changing `birthday` to your birthday.

```{r}
num_sims = 2500
p_val_1 = rep(0, num_sims)
p_val_2 = rep(0, num_sims)
birthday = 19730321
set.seed(birthday)
```

Repeat the above process of generating `y_1` and `y_2` as defined above, and fit models with each as the response `2500` times. Each time, store the p-value for testing,

\[
\beta_2 = 0,
\]

using both models, in the appropriate variables defined above. (You do not need to use a data frame as we have in the past. Although, feel free to modify the code to instead use a data frame.)

```{r}
for(i in 1:num_sims){
  y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = 1)
  fit_1 = lm(y_1 ~ x_1 + x_2)
  p_val_1[i] <-  coef(summary(fit_1))["x_2", "Pr(>|t|)"]
  
  
  y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = abs(x_2))
  fit_2 = lm(y_2 ~ x_1 + x_2)
  p_val_2[i] <- coef(summary(fit_2))["x_2", "Pr(>|t|)"]
}
```


<br><br>

**(b)** What proportion of the `p_val_1` values is less than 0.01? Less than 0.05? Less than 0.10? What proportion of the `p_val_2` values is less than 0.01? Less than 0.05? Less than 0.10? Arrange your results in a table. Briefly explain these results.

```{r fig.width=15}

p_val_1_proportions <- c(mean(p_val_1<.01), mean(p_val_1<.05), mean(p_val_1<.10))
p_val_2_proportions <- c(mean(p_val_2<.01), mean(p_val_2<.05), mean(p_val_2<.10))

df1 <- rbind(p_val_1_proportions, p_val_2_proportions)
colnames(df1) <- c("Proportion pval<.01", "Proportion pval<.05", "Proportion pval<.10")
rownames(df1) <- c("p_val for model 1 where sd = 1", "p_val for model 2 where sd = abs(x_2)")

knitr::kable(df1)

```

In order to explain the results we also plot the distributions of these p-values


```{r fig.width=15}
par(mfrow = c(1,2))

hist(p_val_1, main = "p-value distribution for 1st model", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "p-value", xlim = c(0,max(p_val_1)))

hist(p_val_2, main = "p-value distribution for 2nd model", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "p-value", xlim = c(0,max(p_val_2)))
```

Based on the table and distributions we see that the 1st model has a uniform distribution which is why the table shows that the proportion of p-values less than a certain percentage of distribution is almost equal to the percentage. This implies that there is no significant relationship between $\beta_2$ and the response variable and that it supports the Null Hypothesis. This finding matches with the true model where we see that $\beta_2 = 0$.

For the 2nd model, we see that p-value distribution is closer to 0, and in the table we see that there is a higher proportion of p-value less than a given percentage. This implies that there is increasing evidence that Null Hypothesis can be rejected and that there may be a significant relationship between $\beta_2$ and the response variable. This does not match what we know to be true from our model and hence we can conclude that if the linear model assumptions are not met, the Hypothesis test results will be incorrect. Garbage in = Garbage out. Model where linear model assumptions are not met = incorrect result of Hypothesis tests regarding significance of relationship


***

## Exercise 4 (Corrosion Data)

For this exercise, we will use the `corrosion` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?corrosion` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit a simple linear regression with `loss` as the response and `Fe` as the predictor. Plot a scatterplot and add the fitted line. Check the assumptions of this model.

```{r fig.width=15, message=FALSE}
m1 <- lm(formula = loss~Fe, data = corrosion)

ggplot(data = m1, aes(y=loss, x=Fe)) + geom_point() +stat_smooth(method = lm, se = FALSE) + ggtitle("Scatter plot to show relationship between loss and Iron content from corrosion dataset") +  theme_light()

diagnostics2(m1, pcol = "black")

```

Looking at the QQ plot it looks like the normal distribution assumptions are met. Looking at the residuals plot we see that in the middle the points are below the line and for lower and higher fitted values the points are generally above the line. This tells us that there is a pattern that we are missing and a higher order transformation might be useful


<br><br>

**(b)** Fit higher order polynomial models of degree 2, 3, and 4. For each, plot a fitted versus residuals plot and comment on the constant variance assumption. Based on those plots, which of these three models do you think are acceptable? 

```{r fig.width=15}
m2 <- lm(formula = loss ~ Fe + I(Fe^2), data = corrosion)
diagnostics2(m2, pcol = "black")

m3 <- lm(formula = loss ~ Fe + I(Fe^2) + I(Fe^3), data = corrosion)
diagnostics2(m3, pcol = "black")

m4 <- lm(formula = loss ~ Fe + I(Fe^2) + I(Fe^3) + I(Fe^4), data = corrosion)
diagnostics2(m4, pcol = "black")
```

Looking at the plots we see that plot for model 3 seem to display constant variance. Plot for model 2 seems to have a pattern with points in the middle below the line and for lower and higher fitted values above the line. Model 4 seems to have more points below the line than above the line. Hence we prefer the third order model

Use a statistical test(s) to compare the models you just chose. Based on the test, which is preferred? Check the normality assumption of this model. 

We do an anova test between the models to see which model is significant

```{r}
anova(m1, m2)
```

Looking at the anova result we prefer the 1st model. Now we test the 1st and the 3rd model

```{r}
anova(m1, m3)
```

Looking at the result we prefer the 3rd model. 

To validate that third model is indeed better than 2nd model we do an anova test between 2nd and 3rd model

```{r}
anova(m2, m3)
```

Since p-value is small we prefer the 3rd model


Now we do anova test for 3rd and 4th model

```{r}
anova(m3, m4)
```

Based on the result and visual inspection of the plots,  we still prefer the third model

We now do the Normality and Equal variance test for the third model

```{r fig.width=15}
diagnostics2(m3, plotit = FALSE)
```

Based on the test results we see that that the Normality and Equal variance assumptions are met

We also look at the fitted regression line with the third model to validate that the line is indeed fitting better

```{r fig.width=15, message=FALSE}
ggplot(data = corrosion, aes(y=loss, x=Fe)) + geom_point() + 
  stat_smooth(method="lm", se=TRUE, fill=NA, formula=y ~ poly(x, 3, raw=TRUE),colour="red") + 
  ggtitle(paste("Scatter plot beteeen loss and iron content with model ", summary(m3)$call)[2])

```


Identify any influential observations of this model.

```{r}

(m3_influentials <- corrosion[names(which(cooks.distance(m3)>4/nrow(corrosion))),])
```

Based on the above we see that there are no influential observations


***

## Exercise 5 (Diamonds)

The data set `diamonds` from the `ggplot2` package contains prices and characteristics of 54,000 diamonds. For this exercise, use `price` as the response variable $y$, and `carat` as the predictor $x$. Use `?diamonds` to learn more.

**(a)** Fit a linear model with `price` as the response variable $y$, and `carat` as the predictor $x$. Return the summary information of this model.

```{r}
m1 <- lm(formula = price~carat, data = diamonds)
summary(m1)
```

<br><br>

**(b)** Plot a scatterplot of price versus carat and add the line for the fitted model in part **(a)**. Using a fitted versus residuals plot and/or a Q-Q plot, comment on the diagnostics. 

```{r fig.width=15, message=FALSE}
ggplot(data = m1, aes(y=price, x=carat)) + geom_point() + stat_smooth(method = lm, se = FALSE) + ggtitle("Scatter plot to show relationship between price and carat from diamonds dataset") +  theme_light()

diagnostics2(m1, testit = FALSE)

```

Based on the above plots we make the following observations

1) The linear line does not seem to be fitting the data very well
2) The normality assumption seems suspect based on the QQ plot
3) The Equal variance assumptions seems to be violated when looking at the residuals vs fit plot

<br><br>

**(c)** Seeing as the price stretches over several orders of magnitude, it seems reasonable to try a log transformation of the response. Fit a model with a logged response, plot a scatterplot of log-price versus carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r}
qplot(price, data = diamonds, bins = 30)
```
```{r fig.width=15, message=FALSE}
m2 <- lm(formula = log(price)~carat, data = diamonds)

ggplot(data = m2, aes(y=`log(price)`, x=carat)) + geom_point() +stat_smooth(method = lm, se = FALSE) + ggtitle("Scatter plot to show relationship between log(price) and carat from diamonds dataset") +  theme_light()

diagnostics2(m2, testit = FALSE)

```

looking at the diagnostic charts, the QQ and residual vs fitted charts, we see that both Normality and equal variance assumptions are suspect. We do notice that the upper tail of the QQ plot has improved after converting price to log when compared to the previous plot.

<br><br>

**(d)** Try adding log transformation of the predictor. Fit a model with a logged response and logged predictor, plot a scatterplot of log-price versus log-carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r fig.width=15, message=FALSE}
m3 <- lm(formula = log(price)~log(carat), data = diamonds)

ggplot(data = m3, aes(y=`log(price)`, x=`log(price)`)) + geom_point() +stat_smooth(method = lm, se = FALSE) + ggtitle("Scatter plot to show relationship between log(price) and log(carat) from diamonds dataset") +  theme_light()

diagnostics2(m3, testit = FALSE)

```

The diagnostics of the model have improved a lot from the original dataset where the Normal assumption seems less suspect with both the tails coming in, and the constant variance plot seems a lot better, but they still dont seem to be perfect though.

<br><br>

**(e)** Use the model from part **(d)** to predict the price (in dollars) of a 3-carat diamond. Construct a 99% prediction interval for the price (in dollars).

```{r}
newdata <- data.frame("carat"=c(3))
(p1 <- exp(predict(m3, newdata = newdata, interval = c("predict"), level = .99)))
```



