---
title: "Week 9 - Homework"
author: "STAT 420, Summer 2020, purohit4@illinois.edu"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

## Exercise 1 (`longley` Macroeconomic Data)

The built-in dataset `longley` contains macroeconomic data for predicting employment. We will attempt to model the `Employed` variable.

```{r, eval = FALSE, include=FALSE}
View(longley)
?longley
```

<br><br>

**(a)** What is the largest correlation between any pair of predictors in the dataset?
```{r results="hold"}
c <- cor(longley)
indices <- which(c == max(c[c<1]), arr.ind = TRUE)
rownames(indices)
c[rownames(indices)[1],rownames(indices)[2]]

```
The largest correlation is between Year and GNP and it is .9953. 

<br><br>

**(b)** Fit a model with `Employed` as the response and the remaining variables as predictors. Calculate and report the variance inflation factor (VIF) for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?
```{r message=FALSE}
library(car)

m1 <- lm(Employed~., data = longley)
(vif_longley <- vif(m1))
vif_longley[which.max(vif_longley)]
```
GNP has the largest VIF. 

Except Armed.Forces all VIFs suggest multicollinearity since their values are greater than 5

<br><br>

**(c)** What proportion of the observed variation in `Population` is explained by a linear relationship with the other predictors?
```{r}
summary(lm(Population~.-Employed, data = longley))$"r.squared"
```
99.75% of the observed variation in Population is explained by the other predictors

<br><br>

**(d)** Calculate the partial correlation coefficient for `Population` and `Employed` **with the effects of the other predictors removed**.

```{r}
cor(resid(lm(Population~.-Employed, data = longley)), resid(lm(Employed~.-Population, data = longley)))
```

todo - is this answer correct?

<br><br>

**(e)** Fit a new model with `Employed` as the response and the predictors from the model in **(b)** that were significant. (Use $\alpha = 0.05$.) Calculate and report the variance inflation factor for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?


```{r results="hold"}
c_m1 <- coef(summary(m1))
preds_1 <- which(c_m1[,"Pr(>|t|)"] < .05)
pred_names <- names(preds_1)[2:length(preds_1)]
cat(paste("predictors that are significant from model in 'b' are ", paste(pred_names, collapse = ", "), "\n\n"))
pred_formula <- paste(pred_names, collapse = "+")
full_formula <- paste("Employed~", pred_formula, sep = "")
m2 <- lm(full_formula, data = longley)
cat("variance inflation factors for each of the predictors are as below \n")
(vif_m2 <- vif(m2))
cat("\n")
cat(paste("The variable with the largest vif = ", names(vif_m2[max(vif_m2)]), "\n"))


```
None of the VIFs suggest multicollinearity since they are all less than 5

<br><br>

**(f)** Use an $F$-test to compare the models in parts **(b)** and **(e)**. Report the following:


```{r}
(anova_result <- anova(m2,m1))
```


- The null hypothesis

The null Hypothesis is 
$H_0 : \hat{Employed} = \beta_0 + \beta_1 Unemployed + \beta_2 Armed.Forces + \beta_3 Year + \epsilon$
or that Employed can be expressed in terms of Unemployed and Armed.Forces and Year

todo- is the null hypothesis written correctly

- The test statistic
```{r}
anova_result$F[2]
```

- The distribution of the test statistic under the null hypothesis

The distribution of the test statistic under null hypothesis will be an F distribution with 3 degrees of freedom


- The p-value

```{r}
anova_result$`Pr(>F)`[2]
```

- A decision

Since p value is greater than any reasonable $\alpha$ we Fail to reject the null Hypothesis

- Which model you prefer, **(b)** or **(e)**

We prefer the model from **(e)**

<br><br>

**(g)** Check the assumptions of the model chosen in part **(f)**. Do any assumptions appear to be violated?
```{r fig.width=15, message=FALSE}
library(ggplot2)
library(gridExtra)
library(lmtest)
```

```{r fig.width=15, echo=TRUE}
diagnostics <- function(model = fit_1, pcol = 'dodgerblue', lcol = 'red', alpha = .05, plotit = TRUE, testit = TRUE){
  
  if(plotit == TRUE){
    g1 <- ggplot(data = model, aes(sample=.resid)) + 
      stat_qq(color=I(pcol)) + stat_qq_line(color = I(lcol)) +
      ggtitle("Normal QQ Plot") +  theme_light() 
    
    g2 <- ggplot(data = model, aes(x = fitted(model), y = resid(model))) +
      geom_point(color=I(pcol)) + geom_hline(yintercept=0, color = I(lcol)) +
      xlab("Fitted") + ylab("Residuals") + ggtitle("Residuals vs Fitted Plot") + theme_light() 
    
    grid.arrange(g1, g2, ncol=2)
  }
  
  if(testit == TRUE){
    pval <- shapiro.test(resid(model))$"p.value"
    decision <- "Fail to Reject."
    if(pval < alpha){
      decision <- "Reject"
    }
    shapiro_Normalcy_test_result <- list(p_val=pval, decision=decision)
    
    bptest(model)
    pval <-  bptest(model)$"p.value"
    decision <- "Fail to Reject."
    if(pval < alpha){
      decision <- "Reject"
    }
    bptest_Const_Variance_test_result <- list(p_val=pval, decision=decision)
    df1 <- rbind(shapiro_Normalcy_test_result, bptest_Const_Variance_test_result)
    
    return(df1)
  }
}

```

```{r fig.width=15}
diagnostics(m2)
```

The plots seem to suggest that the constant variance assumption may be suspect. The QQ plot also shows a fat tail on the lower end which encourages us to get a more definitive answer. To be sure we look at the results of the Shapiro tests and BP test. Based on the test results, we conclude that the Normal distribution of errors and constant variance assumptions of the model are not violated. The model is fitting using a linear expression hence the linearity assumption is also not violated. We dont have enough data on the Indendent observations assumption, and one could argue that one year is dependent on the previous. But for the sake of this exercise we will assume that the independent assumption is also met.

todo - is this correct. Using the test results to conclude even though the plots seem suspect. Should we hide the function with echo=FALSE?

***

## Exercise 2 (`Credit` Data)

For this exercise, use the `Credit` data from the `ISLR` package. Use the following code to remove the `ID` variable which is not useful for modeling.

```{r message=FALSE}
library(ISLR)
library(car)
```


```{r, include=FALSE, eval=FALSE}
?Credit
summary(Credit)
summary(lm(Balance ~ Student + Limit, data=Credit))
```


```{r}
data(Credit)
Credit = subset(Credit, select = -c(ID))
```

```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

<br><br>

**(a)** Find a "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `140`
- Obtain an adjusted $R^2$ above `0.90`
- Fail to reject the Breusch-Pagan test with an $\alpha$ of $0.01$
- Use fewer than 10 $\beta$ parameters

Store your model in a variable called `mod_a`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.

Let us look at the pairs plot after removing the factor variables

```{r fig.height=10, fig.width=20}
pairs(Balance~Income+Limit+Rating+Cards+Age+Education,data=Credit)
```

Looking at the above plot a few things stand out

1) Income, Limit and Rating are showing a cluster towards the lower end of the x axis for most pairs, implying that they may benefit from log transformations
2) There is strong colinearity between Limit and Rating and maybe between Income and Limit and Income and Rating

To evaluate if we should transform Income, Limit and Rating with a log transformation, let us compare their relationship plot with Balance before and after transformations


```{r fig.height=7, fig.width=15}
g1 <- ggplot(data = Credit, aes(x=Income, y=Balance))+geom_point()
g2 <- ggplot(data = Credit, aes(x=Limit, y=Balance))+geom_point()
g3 <- ggplot(data = Credit, aes(x=Rating, y=Balance))+geom_point()
g4 <- ggplot(data = Credit, aes(x=log(Income), y=Balance))+geom_point()
g5 <- ggplot(data = Credit, aes(x=log(Limit), y=Balance))+geom_point()
g6 <- ggplot(data = Credit, aes(x=log(Rating), y=Balance))+geom_point()

grid.arrange(g1,g2,g3,g4,g5,g5, ncol=3)
```

Looking at the plot comparisons, we conclude that only Income will benefit from a log transformation. Hence we start with a model with all parameters but Income with log transformation. Then use step to step backwards and find the model with minimum AIC.

todo - is it ok to show this plot or is this considered unnecessary?

```{r}
mod_1 <- lm(Balance ~ (log(Income) + Limit + Rating + Cards + Age + Education + Gender + Student + Married + Ethnicity), data=Credit)
#Find model with minimum AIC and extract formula of model
(frm_1 <- formula(step(mod_1, direction = "backward", trace=0)))

mod_a <- lm(formula = frm_1, data = Credit)
```

Test to see if the model fits the required conditions

```{r}
get_loocv_rmse(mod_a)
get_adj_r2(mod_a)
get_bp_decision(mod_a, alpha = 0.01)
get_num_params(mod_a)
```
todo - is it ok to show a failed test results or should I hide this?

We see that all conditions except Normal distribution of errors is met. Now identify the variables that are passing the BP test as a single linear regression model with Balance as response. Display the variables that do not violate the normal distribution assumption

```{r}
cols <- colnames(Credit)[colnames(Credit)!="Balance"]
j <- 1
normal_error_parameters <- c("")
for(i in cols){
  s <- as.formula(paste("Balance~",i))
  bpt_result <- bptest(lm(s, data = Credit))
  if(bpt_result$"p.value">.01){
    normal_error_parameters[j] <- i
    j <- j+1
  }
}
normal_error_parameters
```

The above variables do not violate the errors are normally distributed assumption when plotted individually with Balance as response

Now check to see if the model we have selected has the above variables as predictors. We see that "Education, Gender, Married and Ethnicity" are not part of the model. It goes to reason that adding these to the model may help with the response not violating the normal distribution assumption.

Start adding the parameters one and a time and see which one helps with the overall model passing all the tests

```{r}
variables_to_try <- c("Education", "Gender", "Married", "Ethnicity")
j <- 1
good_variables <- c("")
for(v in variables_to_try){
  frm_2 <- paste((deparse(frm_1)), "+", v)
  m <- lm(frm_2, data = Credit)
  if(get_loocv_rmse(m)<140 & get_adj_r2(m)>.9 & get_bp_decision(m, alpha = 0.01)!="Reject" & get_num_params(m)<10){
    good_variables[j] <- v
    j <- j+1
  }
}
good_variables
```
The above variables, when made a predictor for a sinle linear regression model with Balance as response, pass all tests

Now adding either of these to our model may make the model meet our condition. We will add the Gender first and run our tests

```{r}
(frm_2 <- paste((deparse(frm_1)), "+", good_variables[1]))
mod_a <- lm(frm_2, data = Credit)
```

```{r}
get_loocv_rmse(mod_a)
get_adj_r2(mod_a)
get_bp_decision(mod_a, alpha = 0.01)
get_num_params(mod_a)
```
We see that the model meets all the conditions

<br><br>

**(b)** Find another "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `130`
- Obtain an adjusted $R^2$ above `0.85`
- Fail to reject the Shapiro-Wilk test with an $\alpha$ of $0.01$
- Use fewer than 25 $\beta$ parameters

Let us take the formula from the above model and add two way interactions. Then we create a model with these two way interactions and use this as the starting model.

Then use the step function and step back from the model to find the model with the lowest AIC

```{r}
mod_1 <- lm("Balance ~ (log(Income) + Limit + Cards + Age + Student + Gender)^2", data = Credit)

#Find model with minimum AIC and extract formula of model
(frm_1 <- formula(step(mod_1, direction = "backward", trace=0)))

mod_b <- lm(formula = frm_1, data = Credit)
```

We take the above model selected by step, and store it in mod_b. We now run the tests given to see if the model meets the given conditions

- Reach a LOOCV-RMSE below `130`
- Obtain an adjusted $R^2$ above `0.85`
- Fail to reject the Shapiro-Wilk test with an $\alpha$ of $0.01$
- Use fewer than 25 $\beta$ parameters

```{r}
get_loocv_rmse(mod_b)
get_adj_r2(mod_b)
get_sw_decision(mod_b, alpha = 0.01)
get_num_params(mod_b)
```
We see that all our conditions are met for the model

***

## Exercise 3 (`Sacramento` Housing Data)

For this exercise, use the `Sacramento` data from the `caret` package. Use the following code to perform some preprocessing of the data.

```{r message=FALSE}
library(caret)
library(ggplot2)
data(Sacramento)
sac_data = Sacramento
sac_data$limits = factor(ifelse(sac_data$city == "SACRAMENTO", "in", "out"))
sac_data = subset(sac_data, select = -c(city, zip)) 

```

Instead of using the `city` or `zip` variables that exist in the dataset, we will simply create a variable (`limits`) indicating whether or not a house is technically within the city limits of Sacramento. (We do this because they would both be factor variables with a **large** number of levels. This is a choice that is made due to laziness, not necessarily because it is justified. Think about what issues these variables might cause.)

Use `?Sacramento` to learn more about this dataset.

A plot of longitude versus latitude gives us a sense of where the city limits are.

```{r}
qplot(y = longitude, x = latitude, data = sac_data,
      col = limits, main = "Sacramento City Limits ")
```

After these modifications, we test-train split the data.

```{r}
set.seed(420)
sac_trn_idx  = sample(nrow(sac_data), size = trunc(0.80 * nrow(sac_data)))
sac_trn_data = sac_data[sac_trn_idx, ]
sac_tst_data = sac_data[-sac_trn_idx, ]
```

The training data should be used for all model fitting. Our goal is to find a model that is useful for predicting home prices.

<br><br>

**(a)** Find a "good" model for `price`. Use any methods seen in class. The model should reach a LOOCV-RMSE below 77,500 in the training data. Do not use any transformations of the response variable.

Let us first look at the pairs plot to understand the data. We remove time and limits from the pairs plot since they are factors. We also remove beds and baths since they have discrete values

```{r fig.height=10, fig.width=15}
pairs(price~sqft+latitude+longitude, data=sac_data)
```
Looking at the pairs plots we want to evaluate if log transformations will help when plotting individually plotting sqft, latitude and longitude with price


```{r fig.height=7, fig.width=15}
g1 <- ggplot(data = sac_data, aes(x=sqft, y=price))+geom_point()
g2 <- ggplot(data = sac_data, aes(x=latitude, y=price))+geom_point()
g3 <- ggplot(data = sac_data, aes(x=longitude, y=price))+geom_point()
g4 <- ggplot(data = sac_data, aes(x=log(sqft), y=price))+geom_point()
g5 <- ggplot(data = sac_data, aes(x=log(latitude), y=price))+geom_point()
g6 <- ggplot(data = sac_data, aes(x=log(longitude), y=price))+geom_point()

grid.arrange(g1,g2,g3,g4,g5,g5, ncol=3)
```
Based on the above plots we see that sqft may benefit from log transformation

We start with a model with all variables including log(sqft) and their two way interactions. Then we step backwards to find a model with the best AIC

```{r results="hold"}
m1 <- lm("price~(log(sqft)+beds+baths+type+latitude+longitude+limits)^2", data = sac_trn_data)

(frm_1 <- formula(step(m1, direction = "backward", trace=0)))

mod_a <- lm(formula = frm_1, data = sac_trn_data)

cat(paste("\n  RMSE for the selected model = ", get_loocv_rmse(mod_a), "\n"))

```
<br><br>

**(b)** Is a model that achieves a LOOCV-RMSE below 77,500 useful in this case? That is, is an average error of 77,500 low enough when predicting home prices? To further investigate, use the held-out test data and your model from part **(a)** to do two things:


- Calculate the average percent error:
\[
\frac{1}{n}\sum_i\frac{|\text{predicted}_i - \text{actual}_i|}{\text{predicted}_i} \times 100
\]


```{r}
pred_prices <- predict(mod_a, newdata = sac_tst_data)
sum(abs(pred_prices - sac_tst_data$price)/pred_prices)/nrow(sac_tst_data)*100
```

- Plot the predicted versus the actual values and add the line $y = x$.

```{r fig.width=15}
ggplot(data=data.frame(predicted=pred_prices, actual=sac_tst_data$price), aes(x=predicted, y=actual)) + 
  geom_point() + geom_abline(intercept=0, slope=1) + ggtitle("Graph showing predicted vs actual prices for test data") +theme_light()
```


Based on all of this information, argue whether or not this model is useful.

The model isnt correct, but is useful. It is more useful for prices below 25K, after which the variation increases and the errors are higher

todo - is this sufficient

***

## Exercise 4 (Does It Work?)

In this exercise, we will investigate how well backwards AIC and BIC actually perform. For either to be "working" correctly, they should result in a low number of both **false positives** and **false negatives**. In model selection,

- **False Positive**, FP: Incorrectly including a variable in the model. Including a *non-significant* variable
- **False Negative**, FN: Incorrectly excluding a variable in the model. Excluding a *significant* variable

Consider the **true** model

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_5 + \beta_6 x_6 + \beta_7 x_7 + \beta_8 x_8 + \beta_9 x_9 + \beta_{10} x_{10} + \epsilon
\]

where $\epsilon \sim N(0, \sigma^2 = 4)$. The true values of the $\beta$ parameters are given in the `R` code below.

```{r}
beta_0  = 1
beta_1  = -1
beta_2  = 2
beta_3  = -2
beta_4  = 1
beta_5  = 1
beta_6  = 0
beta_7  = 0
beta_8  = 0
beta_9  = 0
beta_10 = 0
sigma = 2
```

Then, as we have specified them, some variables are significant, and some are not. We store their names in `R` variables for use later.

```{r}
not_sig  = c("x_6", "x_7", "x_8", "x_9", "x_10")
signif = c("x_1", "x_2", "x_3", "x_4", "x_5")
```

We now simulate values for these `x` variables, which we will use throughout part **(a)**.

```{r}
set.seed(400)
n = 100
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = runif(n, 0, 10)
x_9  = runif(n, 0, 10)
x_10 = runif(n, 0, 10)
```

todo - replace set seed 400 by this here?

We then combine these into a data frame and simulate `y` according to the true model.

```{r}
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```

We do a quick check to make sure everything looks correct.

```{r}
head(sim_data_1)
```

Now, we fit an incorrect model.

```{r}
fit = lm(y ~ x_1 + x_2 + x_6 + x_7, data = sim_data_1)
coef(fit)
```

Notice, we have coefficients for `x_1`, `x_2`, `x_6`, and `x_7`. This means that `x_6` and `x_7` are false positives, while `x_3`, `x_4`, and `x_5` are false negatives.

To detect the false negatives, use:

```{r}
# which are false negatives?
!(signif %in% names(coef(fit)))
```

To detect the false positives, use:

```{r}
# which are false positives?
names(coef(fit)) %in% not_sig
```

Note that in both cases, you could `sum()` the result to obtain the number of false negatives or positives.

<br><br>

**(a)** Set a seed equal to your birthday; then, using the given data for each `x` variable above in `sim_data_1`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table.
```{r}
sim_data_return_Rate <- function(num_simul, mu, sigma, sim_data, n, signif, not_sig){
  false_negs_aic <- rep(num_simul,0)
  false_negs_bic <- rep(num_simul,0)
  false_pos_aic <- rep(num_simul,0)
  false_pos_bic <- rep(num_simul,0)
  
  for(i in 1:num_simul){
    sim_data$y = beta_0 + beta_1 * sim_data$x_1 + beta_2 * sim_data$x_2 + beta_3 * sim_data$x_3 + 
      beta_4 * sim_data$x_4 + beta_5 * sim_data$x_5 + rnorm(n, mu , sigma)
                          
    m1 <- lm("y~.", data=sim_data)
    m_aic <- step(m1, direction = "backward", trace=0)
    false_negs_aic[i] <- sum(!(signif %in% names(coef(m_aic))))
    false_pos_aic[i] <- sum(names(coef(m_aic)) %in% not_sig)
    m_bic <- step(m1, direction = "backward", trace=0, k=log(nrow(sim_data_1)))
    false_negs_bic[i] <- sum(!(signif %in% names(coef(m_bic))))
    false_pos_bic[i] <- sum(names(coef(m_bic)) %in% not_sig)
  }
  l1 <- list("AIC"=mean(false_pos_aic), "BIC"=mean(false_pos_bic))
  l2 <- list("AIC"=mean(false_negs_aic), "BIC"=mean(false_negs_bic))
  df1 <- rbind(l1,l2)
  rownames(df1) <- c("Rate of False Positives", "Rate of False Negatives")
  return(df1)
}
```

```{r message=FALSE}
set.seed(19732103)
num_simul <- 300
df1 <- sim_data_return_Rate(num_simul = num_simul, mu=0, sigma = sigma, sim_data = sim_data_1, n=n, signif = signif, not_sig = not_sig)

colnames(df1) <- c("AIC: sim_data_1", "BIC: sim_data_1")
knitr::kable(df1)

```

<br><br>

**(b)** Set a seed equal to your birthday; then, using the given data for each `x` variable below in `sim_data_2`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table. Also compare to your answers in part **(a)** and suggest a reason for any differences.

```{r}
set.seed(94)
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = x_1 + rnorm(n, 0, 0.1)
x_9  = x_1 + rnorm(n, 0, 0.1)
x_10 = x_2 + rnorm(n, 0, 0.1)

sim_data_2 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```


```{r message=FALSE}
set.seed(19732103)
num_simul <- 300
df2 <- sim_data_return_Rate(num_simul = num_simul, mu=0, sigma = sigma, sim_data = sim_data_2, n=n, signif = signif, not_sig = not_sig)
colnames(df2) <- c("AIC: sim_data_2", "BIC: sim_data_2")
knitr::kable(df2)

```


Comparing the two results

```{r}
knitr::kable(cbind(df1, df2))
```


```{r message=FALSE}
library(GGally)
```

We see that BIC outperforms AIC in the rate of selection of False positives and False negatives. Based on how the sim_dat_2 was simulated, there is multicolinearity  which can be seen by the plot below


```{r fig.height=9, fig.width=15, message=FALSE, eval=TRUE}
ggpairs(sim_data_2, ggplot2::aes(color=I("navyblue")), title = "ggpairs plot to see correlation and distribution of sim_data_2", lower = list(continuous = wrap("smooth")))
```


We note that when multicolinearity increases the performance of both AIC and BIC suffers. We see that BIC is more impacted by collinearity when compared to AIC.

todo- is this sufficient?