---
title: "Week 3 - Homework"
author: "Praveen Purohit : purohit4@illinois.edu"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---


***

## Exercise 1 (Using `lm` for Inference)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Fit the following simple linear regression model in `R`. Use heart weight as the response and body weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `cat_model`. Use a $t$ test to test the significance of the regression. Report the following:

```{r}
library(MASS)
cat_model <- lm(Hwt~Bwt, data = cats)
```

- The null and alternative hypotheses

$H_0 : \beta_1 = 0$ Implying that the response variable does not change based on the predictor variable OR no slope OR there isnt a significant linear relationship between body weight and heart weight of cats

$H_A : \beta_1 \ne 0$ Implying that there is a significant linear relationship between body weight and heart weight of cats


- The value of the test statistic

```{r}
coef(summary(cat_model))[2,3]
```

- The p-value of the test

```{r}
coef(summary(cat_model))[2,4]

```

- A statistical decision at $\alpha = 0.05$

Looking at the p-value above which is essentially 0, we can say that the probability that the observed value of $\beta_1$ being in the 95% confidence interval is almost 0. Also p-value is less than alpha. Hence the statistical decision is to reject the null Hypothesis


- A conclusion in the context of the problem

As conclusion we can say that there is a significant linear relationship between the body weight and heart weight of a cat. We conclude this since our null Hypothesis was that there isnt a significant relationship. Then we looked at the p value OR the probability that the estimated value of $\beta_1$ which is $\hat\beta_1$ is the value by chance given that $\beta_1$ is 0, and the chance or probability of that turned out to be 0. Hence we rejected the null Hypothesis and accepted the alternate Hypothesis that $\beta_1 \ne 0$ and that there is a significant linear relationship between heart weight and body weigh of cats

<br><br>

**(b)** Calculate a 95% confidence interval for $\beta_1$. Give an interpretation of the interval in the context of the problem.

```{r}
confint(cat_model, parm=c("Bwt"), level = .95)
```

The above is the 95% confidence interval for $\beta_1$. This implies that we are 95% confident that for every 1kg increase in body weight of a cat, the heart weight in grams of a cat increases by 3.54gms to 4.53 gms


<br><br>

**(c)** Calculate a 90% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.

```{r}
confint(cat_model, parm=c("(Intercept)"), level = .9)

```
Technically the interpretation of the above confidence iterval is that we are 90% confident that if a cat's body weight is 0kg then the cat's heart weight is between -1.5gms to .78gms. We should look at this conclusion with suspicion since we know that there will be no cat with 0 body weight and obviously a cat's heart cannot have a negative weight.

<br><br>

**(d)** Use a 90% confidence interval to estimate the mean heart weight for body weights of 2.1 and 2.8 kilograms. Which of the two intervals is wider? Why?

```{r results='hold'}
mean_bwt <- mean(cats$Bwt)
print(paste("mean body weight of cats is = ", mean_bwt))

m1  <- predict(cat_model, newdata = data.frame(Bwt=c(2.1, 2.8)),level = .9, interval = c("confidence"))

print(paste("Difference of ", 2.1, " body weight from mean body weight is = ", mean_bwt - 2.1))

print(paste("Difference of ", 2.8, " body weight from mean body weight is = ", mean_bwt - 2.8))

print(paste("Length of confidnece interval for body weight 2.1 is ", diff(c(m1[1,2],m1[1,3]))))

print(paste("Length of confidnece interval for body weight 2.8 is ", diff(c(m1[2,2],m1[2,3]))))

```
We see above that the interval for 2.1kg body weight is wider. We also see that 2.1kg body wight is further away from the mean body weight than 2.8 kg. Since the confidence interval is calculated as 

$point estimate \pm criticaValue * standardError$ 

where standard error is defined as 

$s_e* \sqrt {1/n +(x- \bar x)^2/S_{xx}}$

Given this equation, the bigger the difference between x and $\bar x$, the larger the confidence interval


<br><br>


**(e)** Use a 90% prediction interval to predict the heart weight for body weights of 2.8 and 4.2 kilograms.

```{r results='hold'}
m2  <- predict(cat_model, newdata = data.frame(Bwt=c(2.8, 4.2)),level = .9, interval = c("prediction"))

print(paste("Based on 90% prediction interval, the heart weight for a body weight of 2.8 kgs will be between ", m2[1,2], "gms and ", m2[1,3], "gms"))


print(paste("Based on 90% prediction interval, the heart weight for a body weight of 4.2 kgs will be between ", m2[2,2], "gms and ", m2[2,3], "gms"))

```


<br><br>

**(f)** Create a scatterplot of the data. Add the regression line, 95% confidence bands, and 95% prediction bands.
```{r message=FALSE}
library("ggplot2")

p <- ggplot(data=cats, aes(x=Bwt, y=Hwt)) + geom_point() +
  stat_smooth(method = lm, se=FALSE)+
  ggtitle("Weight of cats heart vs. it body weight") +
  xlab("Body Weight of cats in Kgs") +
  ylab("Heart weight of cats in grams") +
  theme_linedraw()

#simulate x data
x_sim <- seq(min(cats$Bwt), max(cats$Bwt),.1)

c_band <- predict(cat_model, newdata = data.frame(Bwt=x_sim),level = .95, interval = c("confidence"))

p_band <- predict(cat_model, newdata = data.frame(Bwt=x_sim),level = .95, interval = c("prediction"))

p <- p + geom_line(data=data.frame(x=x_sim,y=c(c_band[,2])), aes(x=x_sim, y=c(c_band[,2])), color="red")
p <- p + geom_line(data=data.frame(x=x_sim,y=c(c_band[,3])), aes(x=x_sim, y=c(c_band[,3])), color="red")
p <- p + geom_line(data=data.frame(x=x_sim,y=c(p_band[,2])), aes(x=x_sim, y=c(p_band[,2])))
p <- p + geom_line(data=data.frame(x=x_sim,y=c(p_band[,3])), aes(x=x_sim, y=c(p_band[,3])))
p 

```


<br><br>

**(g)** Use a $t$ test to test:

- $H_0: \beta_1 = 4$
- $H_1: \beta_1 \neq 4$
```{r}
null_beta_1 <- 4
b1_hat_se <- coef(summary(cat_model))[2,2]
b1_hat <- coef(summary(cat_model))[2,1]
```

Report the following:

- The value of the test statistic
```{r}
(t_beta_1_is_4 <- (b1_hat-null_beta_1)/b1_hat_se)

```

We do a 2-tail test below

- The p-value of the test
```{r}
(p_value <- 2*pt(t_beta_1_is_4, df=nrow(cats)-2, lower.tail=FALSE))
```

- A statistical decision at $\alpha = 0.05$
Since the p-value is .89 which is greater than .05 ($\alpha$), our statistical decision is that we Fail to reject the null hypothesis that beta_1 is 4.


***

## Exercise 2 (More `lm` for Inference)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will re-perform the data cleaning done in the previous homework.

```{r}
library(mlbench)
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

**(a)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and wind speed as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_wind_model`. Use a $t$ test to test the significance of the regression. Report the following:
```{r}
ozone_wind_model <- lm(ozone~wind, data = Ozone)
```

- The null and alternative hypotheses

$H_0 : \beta_1 = 0$ Implying that the response variable does not change based on the predictor variable OR no slope OR there isnt a significant linear relationship between wind speed and ozone readings

$H_A : \beta_1 \ne 0$ Implying that there is a significant linear relationship between wind speed and ozone readings


- The value of the test statistic

```{r}
coef(summary(ozone_wind_model))[2,3]
```


- The p-value of the test

```{r}
coef(summary(ozone_wind_model))[2,4]

```

- A statistical decision at $\alpha = 0.01$

Looking at the p-value above which is very high and more than $\alpha$, our statistical decisionis to fail to reject the null Hypothesis. 


- A conclusion in the context of the problem

As conclusion we can say that there is not a significant linear relationship between the wind speed and ozone levels. We conclude this since our null Hypothesis was that there isnt a significant relationship. Then we looked at the p value OR the probability that the estimated value of $\beta_1$ which is $\hat\beta_1$ is the value by chance given that $\beta_1$ is 0, and the chance or probability of that turned out to be 82.7%. Hence we fail to reject the null Hypothesis.


<br><br>

**(b)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and temperature as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_temp_model`. Use a $t$ test to test the significance of the regression. Report the following:
```{r}
ozone_temp_model <- lm(ozone~temp, data = Ozone)
```

- The null and alternative hypotheses

$H_0 : \beta_1 = 0$ Implying that the response variable does not change based on the predictor variable OR no slope OR there isnt a significant linear relationship between temperature and ozone readings 

$H_A : \beta_1 \ne 0$ Implying that there is a significant linear relationship between temperature and ozone readings


- The value of the test statistic

```{r}
coef(summary(ozone_temp_model))[2,3]
```

- The p-value of the test

```{r}
coef(summary(ozone_temp_model))[2,4]

```


- A statistical decision at $\alpha = 0.01$
Looking at the p-value above which is essentially 0 and less than $\alpha$, our statistical decision is to reject the null Hypothesis


- A conclusion in the context of the problem

As conclusion we can say that there is a significant linear relationship between the temperature and ozone readings. We conclude this since our null Hypothesis was that there isnt a significant relationship. Then we looked at the p value OR the probability that the estimated value of $\beta_1$ which is $\hat\beta_1$ is the value by chance given that $\beta_1$ is 0, and the chance or probability of that turned out to be 0. Hence we rejected the null Hypothesis and accepted the alternate Hypothesis that $\beta_1 \ne 0$ and that there is a significant linear relationship between temprature and ozone readings

***

## Exercise 3 (Simulating Sampling Distributions)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = -5$
- $\beta_1 = 3.25$
- $\sigma^2 = 16$

We will use samples of size $n = 50$.

**(a)** Simulate this model $2000$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_0$ and $\hat{\beta}_1$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19730321
set.seed(birthday)
n = 50
x = seq(0, 10, length = n)
```
```{r}
simulate_data <- function(beta_0, beta_1, sd_error, x_values){
  epsilon <- rnorm(n=length(x_values), mean=0, sd = sd_error)
  y <- beta_0 + beta_1*x_values + epsilon
  return(data.frame(x= x_values, y=y))
}

# Create a function to simulate data n times and return the resulting beta_hat_0, beta_hat_1 and se as a list

Simulate_n_times_Get_Beta_hat_0_1_and_se <- function(num_simul, beta_0, beta_1, sd_error, x_values){
  
  beta_hat_0 <-  rep(0,num_simul)
  beta_hat_1 <-  rep(0,num_simul)
  se <-  rep(0,num_simul)

  for(i in 1:num_simul){
    sim_data_1 <- simulate_data(beta_0 = beta_0, beta_1 = beta_1, sd_error = sd_error, x_values = x_values)
    m1 <- lm(y~x,data=sim_data_1)
    
    beta_hat_0[i] <- coef(m1)[1]
    beta_hat_1[i] <- coef(m1)[2]
    se[i] <- summary(m1)$sigma
  }
  return(list(beta_hat_0=beta_hat_0, beta_hat_1=beta_hat_1, se=se))
}
```
```{r}
beta_0 <- -5
beta_1 <- 3.25
sig <- sqrt(16)
ns <- 2000

b_0_1 <- Simulate_n_times_Get_Beta_hat_0_1_and_se(num_simul = ns, beta_0 = beta_0, beta_1 = beta_1, sd_error = sig, x_values = x)
b_hat_0 <- b_0_1$beta_hat_0
b_hat_1 <- b_0_1$beta_hat_1
```


<br><br>


**(b)** Create a table that summarizes the results of the simulations. The table should have two columns, one for $\hat{\beta}_0$ and one for $\hat{\beta}_1$. The table should have four rows:

- A row for the true expected value given the known values of $x$
- A row for the mean of the simulated values
- A row for the true standard deviation given the known values of $x$
- A row for the standard deviation of the simulated values
```{r}
# Calculate True standard deviation of beta_0 and beta_1
Sxx <- sum((x-mean(x))^2)
var_beta_0_hat <- sig^2 * (1/n + (mean(x)^2 / Sxx))
var_beta_1_hat <- sig^2 / Sxx

true_expected_give_x <- list(beta_hat_0=beta_0, beta_hat_1=beta_1)
mean_of_simulated <- list(beta_hat_0=mean(b_hat_0), beta_hat_1=mean(b_hat_1))
true_sd_given_x <- list(beta_hat_0=sqrt(var_beta_0_hat), beta_hat_1=sqrt(var_beta_1_hat))
sd_of_simulated <- list(beta_hat_0=sd(b_hat_0), beta_hat_1=sd(b_hat_1))

library("data.table")

knitr::kable(data.frame(
  rbindlist(list("true expected value given the known values of x"=true_expected_give_x,
            "mean of the simulated values"=mean_of_simulated, 
            "true standard deviation given the known values of x"=true_sd_given_x,
            "standard deviation of the simulated values"=sd_of_simulated), 
            use.names = TRUE, idcol = "Description")), 
            format="markdown")

```


<br><br>

**(c)** Plot two histograms side-by-side:

- A histogram of your simulated values for $\hat{\beta}_0$. Add the normal curve for the true sampling distribution of $\hat{\beta}_0$.
- A histogram of your simulated values for $\hat{\beta}_1$. Add the normal curve for the true sampling distribution of $\hat{\beta}_1$.
```{r fig.width=10, message=FALSE}
library(ggplot2)

b0 <- ggplot(data.frame(b_hat_0), aes(x=b_hat_0)) + 
    geom_histogram(aes(y=stat(density),color=I("orange")), binwidth = .2) +
    ggtitle("beta_hat_0 distribution based on 2000 simulations") +
    theme_linedraw() +
    stat_function(fun = dnorm, args = list(mean = beta_0, sd = sqrt(var_beta_0_hat)), col="red", size=2)

b1 <- ggplot(data.frame(b_hat_1), aes(x=b_hat_1)) + 
    geom_histogram(aes(y=stat(density),color=I("orange")), binwidth = .05) +
    ggtitle("beta_hat_1 distribution based on 2000 simulations") +
    theme_linedraw() +
    stat_function(fun = dnorm, args = list(mean = beta_1, sd = sqrt(var_beta_1_hat)), col="red", size=2)

library("gridExtra")
grid.arrange(b0, b1, ncol=2)
```

***

## Exercise 4 (Simulating Confidence Intervals)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 5$
- $\beta_1 = 2$
- $\sigma^2 = 9$

We will use samples of size $n = 25$.


Our goal here is to use simulation to verify that the confidence intervals really do have their stated confidence level. Do **not** use the `confint()` function for this entire exercise.

**(a)** Simulate this model $2500$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_1$ and $s_e$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19730321
set.seed(birthday)
n = 25
x = seq(0, 2.5, length = n)
```


```{r}
beta_0 <- 5
beta_1 <- 2
sig <-  sqrt(9)
ns <- 2500

b_1_se <- Simulate_n_times_Get_Beta_hat_0_1_and_se(num_simul = ns, beta_0 = beta_0, beta_1 = beta_1, sd_error = sig, x_values = x)

b_hat_1 <- b_1_se$beta_hat_1
b_se <- b_1_se$se
```


<br><br>

**(b)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 95% confidence interval. Store the lower limits in a vector `lower_95` and the upper limits in a vector `upper_95`. Some hints:

- You will need to use `qt()` to calculate the critical value, which will be the same for each interval.
- Remember that `x` is fixed, so $S_{xx}$ will be the same for each interval.
- You could, but do not need to write a `for` loop. Remember vectorized operations.

Confidence interval can be calculated as $ pointEstimate \pm CriticalValue * SE$
```{r}
# Confidence interval can be calculated as $ pointEstimate \pm CriticalValue * SE$
# point estimate is stored in the simulated array result b_hat_1
# Critical value is t statistic for alpha/2 for n-2 df
# SE = s_e/sqrt(Sxx)


Sxx <- sum((x-mean(x))^2)
SE <- b_se/sqrt(Sxx)

crit_val <- qt(.05/2, df=n-2, lower.tail = FALSE)
margin_error <- crit_val*SE

lower_95 <- b_hat_1 - margin_error
upper_95 <- b_hat_1 + margin_error

```


<br><br>

**(c)** What proportion of these intervals contains the true value of $\beta_1$?
```{r}
mean((lower_95 <= beta_1) & (beta_1 <= upper_95))
```

<br><br>

**(d)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.05$?
```{r}
1-mean((lower_95 <= 0) & (0 <= upper_95))

```

<br><br>

**(e)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 99% confidence interval. Store the lower limits in a vector `lower_99` and the upper limits in a vector `upper_99`.
```{r}
crit_val <- qt(.01/2, df=n-2, lower.tail = FALSE)
margin_error <- crit_val*SE

lower_99 <- b_hat_1 - margin_error
upper_99 <- b_hat_1 + margin_error
```


<br><br>

**(f)** What proportion of these intervals contains the true value of $\beta_1$?
```{r}
mean((lower_99 <= beta_1) & (beta_1 <= upper_99))
```

<br><br>

**(g)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.01$?
```{r}
1 - mean((lower_99 <= 0) & (0 <= upper_99))
```

***

## Exercise 5 (Prediction Intervals "without" `predict`)

Write a function named `calc_pred_int` that performs calculates prediction intervals:

$$
\hat{y}(x) \pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{1 + \frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}.
$$

for the linear model

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$

**(a)** Write this function. You may use the `predict()` function, but you may **not** supply a value for the `level` argument of `predict()`. (You can certainly use `predict()` any way you would like in order to check your work.)

The function should take three inputs:

- `model`, a model object that is the result of fitting the SLR model with `lm()`
- `newdata`, a data frame with a single observation (row)
    - This data frame will need to have a variable (column) with the same name as the data used to fit `model`.
- `level`, the level (0.90, 0.95, etc) for the interval with a default value of `0.95`

The function should return a named vector with three elements:

- `estimate`, the midpoint of the interval
- `lower`, the lower bound of the interval
- `upper`, the upper bound of the interval


```{r}

calc_pred_int <- function(model, newdata, level=.95){
  
  alpha <- 1-level
  y_hat <- predict(model, newdata = newdata)
  n <- nrow(model$model)
  xName <- colnames(newdata)
  x_bar <- mean(model$model[,xName])
  crit_val <- qt(alpha/2, df=n-2, lower.tail = FALSE)
  se <- summary(model)$sigma
  Sxx <- sum((model$model[,xName] -x_bar)^2)

  SE <- se * sqrt(1 + 1/n + ((newdata-x_bar)^2 /Sxx))

  lower <- (y_hat - crit_val*SE)[,1]
  upper <- (y_hat + crit_val*SE)[,1]
  
  return (c(estimate=y_hat, lower=lower, upper=upper))
}
```


<br><br>

**(b)** After writing the function, run this code:

```{r}
newcat_1 = data.frame(Bwt = 4.0)
calc_pred_int(cat_model, newcat_1)

```

<br><br>

**(c)** After writing the function, run this code:

```{r}
newcat_2 = data.frame(Bwt = 3.3)
(calc_pred_int(cat_model, newcat_2, level = 0.90))
```


