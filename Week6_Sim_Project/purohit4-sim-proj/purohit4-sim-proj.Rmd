---
title: 'Week 6 - Simulation Project'
author: "STAT 420, Summer 2020, purohit4@illinois.edu"
date: ''
output:
  bookdown::html_document2: 
    toc: true
  bookdown::pdf_document2: 
    toc: true
   
fig_caption: yes
urlcolor: cyan
---

\usepackage{subfig}
***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

# Simulation Study 1: Significance of Regression

## Introduction

In this simulation study we will investigate the significance of regression test. We will simulate from two different models: Significant model \[Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i\] and a corresponding Non Significant     model \[Y_i = \beta_0 + \epsilon_i\]

We will look at two things

1) Investigate how the F-statistic, p-value and $R^2$ changes across the above two models

2) Investigte how changing the sigma value from 1 to 5 to 10, changes the F-statistic, p-value and $R^2$ within the same model, and across the Significant and non significant model

Based on the above investigations we will try and draw conclusions on what effect the changes to beta parameters and sigma have on the F-statistic, p-value

We will share the steps taken to do the analysis in the Mehods section, the results of our steps in the Results section and thoughts and conclusion in the Discussion section



## Method

In this section we will walk through the steps taken to do the simulations across the Significant and the Non Significant model. We will also vary the simulated data for three different sigma values $\sigma = {1, 5, 10}$ and then analyze the impact to F-statistic, p-value and $R^2$ for each of the 6 simulations


```{r message=FALSE, warning=FALSE, include=FALSE}
library(broom)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(statsr)
```

We set the seed to ensure reproducible results
```{r}
birthday = 19730321
set.seed(birthday)
```

```{r}

x_predictors_df <- read.csv("study_1.csv")

n <- 25
mu <- 0
num_simul <- 2000

#Significant model
beta_0 <- 3
beta_1 <- 1
beta_2 <- 1
beta_3 <- 1

```

Library loading and variable initialization not shown for brevity of report. Please refer to Rmd file if required. We initialize the betas to 3,1,1,1. The number of simulations to 2000. mu to 0 and sample size (n) to 25.

We create a function that will simulate the data 2000 times and return the f_statistic, p-value and $R^2$ for each simulation as a dataframe

```{r}
sim_data <- function(sample_size, mu, sig, num_simul, x_predictors_df, non_sig_model){
  #initialize the vectors so that we dont grow them in the for loop
  sim_f_stats <- rep(0,num_simul)
  sim_pvals <- rep(0,num_simul)
  sim_r2s <- rep(0,num_simul)

  
  for (i in 1:num_simul){
    #create random noise based on a normal distribution
    epsilon <- rnorm(sample_size, mean=mu, sd=sig)
    if(non_sig_model==TRUE)
      x_predictors_df$y <- beta_0 + epsilon #For non significant model all predictor betas are 0 hence only beta_0 has an impact
    else
      x_predictors_df$y <- beta_0 + x_predictors_df$x1*beta_1 + x_predictors_df$x2*beta_2 + x_predictors_df$x3*beta_3 + epsilon
    
    m <- lm(y~., data = x_predictors_df)
    
    sim_f_stats[i] <- glance(m)$statistic
    sim_pvals[i] <- glance(m)$p.value
    sim_r2s[i] <- glance(m)$r.squared
  }
  #retuen a dataframe with f-statistic, p-value and R2 from our simulation
  return(data.frame("f_stat"=sim_f_stats, "pval"=sim_pvals, "r2"=sim_r2s))
}
```

Now we Simulate Significant and Non significant models for $\sigma = 1$

```{r}
sig <- 1
Significant_sigma_1_df <- sim_data(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, non_sig_model=FALSE)

Nonsignificant_sigma_1_df <- sim_data(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, non_sig_model=TRUE)
```

We Simulate Significant and Non significant models for $\sigma = 5$

```{r}

sig <- 5
Significant_sigma_5_df <- sim_data(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, non_sig_model=FALSE)

Nonsignificant_sigma_5_df <- sim_data(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, non_sig_model=TRUE)

```

Now we Simulate Significant and Non significant models for $\sigma = 10$

```{r}
sig <- 10
Significant_sigma_10_df <- sim_data(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, non_sig_model=FALSE)

Nonsignificant_sigma_10_df <- sim_data(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, non_sig_model=TRUE)
```


## Results

In this section we will look at the results of the simulation and compare them.

Let us look at each of the model and sigma simulations to see how many times the Null Hypothesis got rejected based on $\alpha = .05$

```{r statcomp}

comp_df <- rbind("Sigma = 1"=c(mean(Significant_sigma_1_df$pval<.05), mean(Nonsignificant_sigma_1_df$pval<.05)),
                 "Sigma = 5"=c(mean(Significant_sigma_5_df$pval<.05), mean(Nonsignificant_sigma_5_df$pval<.05)),
                 "Sigma = 10"=c(mean(Significant_sigma_10_df$pval<.05), mean(Nonsignificant_sigma_10_df$pval<.05)))

colnames(comp_df) <- c("Significant model", "Non-Significant model")


knitr::kable(data.frame(comp_df), caption = "Comparison of Proportion of times $H_0$ was rejected")
```

We will discuss the implications of the above table in the Discussions section

We now plot the distributions of the F-statistic returned from our 6 different simulations. The label for each graph talks about what the graph represents

```{r fstatcomp, fig.height=7, fig.width=15, fig.cap="Figure 1.1 - Comparison of F-Statistic across Significant and Non-Significant model with Sigma in (1,5,10)"}
par(mfrow = c(2,3))

hist(Significant_sigma_1_df$f_stat, main = "Fstat distribution for Significant model - sigma 1", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "F-statistic", xlim = c(0,max(Significant_sigma_1_df$f_stat)))
x <- Significant_sigma_1_df$f_stat
curve( df(x, df1 = 4 - 1, df2 = 25 -4), col = "darkorange", add = TRUE, lwd = 3)

hist(Significant_sigma_5_df$f_stat, main = "Fstat distribution for Significant model - sigma 5", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "F-statistic", xlim = c(0,max(Significant_sigma_5_df$f_stat)))
x <- Significant_sigma_5_df$f_stat
curve( df(x, df1 = 4 - 1, df2 = 25 -4), col = "darkorange", add = TRUE, lwd = 3)

hist(Significant_sigma_10_df$f_stat, main = "Fstat distribution for Significant model - sigma 10", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "F-statistic", xlim = c(0,max(Significant_sigma_10_df$f_stat)))
x <- Significant_sigma_10_df$f_stat
curve( df(x, df1 = 4 - 1, df2 = 25 -4), col = "darkorange", add = TRUE, lwd = 3)

# Non Significant Model curves

hist(Nonsignificant_sigma_1_df$f_stat, main = "Fstat distribution for Non-Significant model - sigma 1", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "F-statistic", xlim = c(0,max(Nonsignificant_sigma_1_df$f_stat)))
x <- Nonsignificant_sigma_1_df$f_stat
curve( df(x, df1 = 4 - 1, df2 = 25 -4), col = "darkorange", add = TRUE, lwd = 3)

hist(Nonsignificant_sigma_5_df$f_stat, main = "Fstat distribution for Non-Significant model - sigma 5", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "F-statistic", xlim = c(0,max(Nonsignificant_sigma_5_df$f_stat)))
x <- Nonsignificant_sigma_5_df$f_stat
curve( df(x, df1 = 4 - 1, df2 = 25 -4), col = "darkorange", add = TRUE, lwd = 3)

hist(Nonsignificant_sigma_10_df$f_stat, main = "Fstat distribution for Non=Significant model - sigma 10", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "F-statistic", xlim = c(0,max(Nonsignificant_sigma_10_df$f_stat)))
x <- Nonsignificant_sigma_10_df$f_stat
curve( df(x, df1 = 4 - 1, df2 = 25 -4), col = "darkorange", add = TRUE, lwd = 3)

```


We will discuss the findings from the above graphs in the discussion section below


Now we plot the distributions of the p-value returned from our 6 different simulations. The label for each graph talks about what the graph represents

```{r pvalcomp, fig.height=7, fig.width=15, fig.cap="Figure 1.2 - Comparison of p-value across Significant and Non-Significant model with Sigma in (1,5,10)"}
par(mfrow = c(2,3))

hist(Significant_sigma_1_df$pval, main = "p-value distribution for Significant model - sigma 1", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "p-value", xlim = c(0,max(Significant_sigma_1_df$pval)))
x <- Significant_sigma_1_df$pval

hist(Significant_sigma_5_df$pval, main = "p-value distribution for Significant model - sigma 5", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "p-value", xlim = c(0,max(Significant_sigma_5_df$pval)))
x <- Significant_sigma_5_df$pval

hist(Significant_sigma_10_df$pval, main = "p-value distribution for Significant model - sigma 10", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "p-value", xlim = c(0,max(Significant_sigma_10_df$pval)))
x <- Significant_sigma_10_df$pval

# Non Significant Model curves

hist(Nonsignificant_sigma_1_df$pval, main = "p-value distribution for Non-Significant model - sigma 1", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "p-value", xlim = c(0,max(Nonsignificant_sigma_1_df$pval)))
x <- Nonsignificant_sigma_1_df$pval
curve( dunif(x, min = min(x), max=max(x)), col = "red", add = TRUE, lwd = 3)

hist(Nonsignificant_sigma_5_df$pval, main = "p-value distribution for Non-Significant model - sigma 5", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "p-value", xlim = c(0,max(Nonsignificant_sigma_5_df$pval)))
x <- Nonsignificant_sigma_5_df$pval
curve( dunif(x, min = min(x), max=max(x)), col = "red", add = TRUE, lwd = 3)

hist(Nonsignificant_sigma_10_df$pval, main = "p-value distribution for Non-Significant model - sigma 10", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "p-value", xlim = c(0,max(Nonsignificant_sigma_10_df$pval)))
x <- Nonsignificant_sigma_10_df$pval
curve( dunif(x, min = min(x), max=max(x)), col = "red", add = TRUE, lwd = 3)
```

Please see the Discussion section below for analysis of the above graphs

As a final step in our Methods section, we plot the distribution of $R^2$ for the 6 models that we have simulated above.

```{r R2comp, fig.height=7, fig.width=15, fig.cap="Figure 1.3 - Comparison of $R^2$ across Significant and Non-Significant model with Sigma in (1,5,10)"}

# Todo should we hide the code for charts and tables?
# Todo is having no discussion in the charts section ok? I am saying we will discuss in the Discussion section and am referencing the charts there

par(mfrow = c(2,3))

hist(Significant_sigma_1_df$r2, main = "$R^2$ distribution for Significant model - sigma 1", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "$R^2$", xlim = c(0,max(Significant_sigma_1_df$r2)))

hist(Significant_sigma_5_df$r2, main = "$R^2$ distribution for Significant model - sigma 5", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "$R^2$", xlim = c(0,max(Significant_sigma_5_df$r2)))

hist(Significant_sigma_10_df$r2, main = "$R^2$ distribution for Significant model - sigma 10", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "$R^2$", xlim = c(0,max(Significant_sigma_10_df$r2)))

# Non Significant Model curves

hist(Nonsignificant_sigma_1_df$r2, main = "$R^2$ distribution for Non-Significant model - sigma 1", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "$R^2$", xlim = c(0,max(Nonsignificant_sigma_1_df$r2)))


hist(Nonsignificant_sigma_5_df$r2, main = "$R^2$ distribution for Non-Significant model - sigma 5", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "$R^2$", xlim = c(0,max(Nonsignificant_sigma_5_df$r2)))

hist(Nonsignificant_sigma_10_df$r2, main = "$R^2$ distribution for Non-Significant model - sigma 10", prob = TRUE, col = "dodgerblue", border = "darkorange", xlab = "$R^2$", xlim = c(0,max(Nonsignificant_sigma_10_df$r2)))

```



```{r}

```


Under one of the models we can come up with a name. How do the distributions behave.
Teh statistics behave differently based on significant or non significant models. F statistic may follow F distribution regarding which model - significnat or non significant?
For R2 say things about skewness and symmetry. Distribution of p value for one of the 2 models will look fimiliar
DOnt need to say why the distribution is certain way. Dont need to know the names of the distributsion. See if vectors or dataframes is better


## Potential discussions:

The above simulations and the charts are telling on what impact the noise (sigma) and signal (significant parameters) have on our ability to predict.

At the high level, it is clear that as the sigma increases, noise (epsilon) increases. The more the noise, the lesser impact the signal has on our ability to predict.

### Impact of signal changing with $\sigma = 1$

Let us first examine the impact of signal on our ability to predict. 

We will assume that sigma or noise is constant in this discussion so that we can clearly see the impact of the beta parameters in the significant model compared to the Non-significant model

In all the three figures, Figure \@ref(fig:fstatcomp), Figure \@ref(fig:pvalcomp) and Figure \@ref(fig:R2comp), we will compare the top and bottom charts for this discussion, since the difference between the top and bottom charts is that the top charts, all the beta parameters are in play and in the bottom charts, only the intercept beta is in play. 

It is easiest to compare the charts of sigma = 1 for this discussion, so that noise($\sigma$) does not clutter our findings. Hence we look at them first

When we compare the significant (top) and Non-sgnificant (bottom) chart for sigma =1, in Figure \@ref(fig:fstatcomp), we make the below observations

* F-statistic is higher for the significant model. Now F-statistic can be thought of as $F=$  $\text{Mean Square Explained} \over \text{Mean Square Unexplained}$  $=$  $\text{Mean Square Regression} \over \text{Mean Square Error}$  $=$  ${\sum_{i=1}^{𝑛} (\hat{y}_{1i} - \bar{y})^2/p-1 \over \sum_{i=1}^{𝑛} (y_i - \hat{y}_{1i})^2/n-p}$. Hence F-statistic will be high if either the predictors are able to explain the regression OR if the error is high. Since we have fixed $\sigma=1$, we know error or variance is not high. Hence the only explanation for the F-statistic being high in the Significant model is that the model is able to explain the regression. Said differently we can say that for $\sigma=1$ we see that atleast one of the predictors has a significant relationship with the response variable.We will discuss the comparison for other sigmas in section 1.4.2


* If we look at Table \@ref(tab:statcomp) we see that for the Significant model, for sigma 1, the Null Hypothesis was rejected 100% of times. This conclusion is the same that we have drawn after looking at the F-statistic chart above


* Let us now see if the p-value chart in Figure \@ref(fig:pvalcomp) corroborates the above finding. Looking at the top left chart, we see that p-value is 0 for all our simulation runs (significant model, sigma=1). This implies that the Null Hypothesis should be rejected. This implies that we accept the alternate Hypothesis that there is a significant linear relationship between atleast one of the predictors and the response variable

* If the above three theories are correct then our chart for $R^2$ should show $R^2$ value to be very high as most of the variability in the Significant model should be explained by our model when sigma=1. When looking at Figure \@ref(fig:R2comp) we find that to be the case in the top left chart where sigma=1 for the Significant model.



### Impact of noise changing ($\sigma$ change impact)

Now we will focus on the change we see when we move from left to right in the charts in our three figures, Figure \@ref(fig:fstatcomp), Figure \@ref(fig:pvalcomp) and Figure \@ref(fig:R2comp). As we move from left to right the noise increases and the impact of signal decreases. This implies that our ability to reject the Null Hypothesis, even if it not true, decreases. To summarize, noise (high $\sigma$) is bad for our ability to predict :)

Let us see if the charts and tables support the above statement.

* If we look at Table \@ref(tab:statcomp) we see that as the sigma increases for the significant model, the Power or the %age of times we reject the Null Hypothesis decreases. Implying if we knew that the Significant model was true, and we simulated the data, our ability to reject the Null hypothesis for the significant model would reduce as sigma increased. Hence for higher levels of sigma, we would make more Type II errors by not rejecting the Null Hypothesis even though the Alternate Hypothesis is true. 

For the Non significant model, if we assume the Model to be true, then it does not change the Power by much since we are starting off by saying that there is no relationship between the predictors and response variables.

* The above theory seems to hold ground when we look at sigma changing from left to right in Figure \@ref(fig:fstatcomp). $F=$  $\text{Mean Square Explained} \over \text{Mean Square Unexplained}$  $=$  $\text{Mean Square Regression} \over \text{Mean Square Error}$  $=$  ${\sum_{i=1}^{𝑛} (\hat{y}_{1i} - \bar{y})^2/p-1 \over \sum_{i=1}^{𝑛} (y_i - \hat{y}_{1i})^2/n-p}$. Now as sigma increases, the denominator increases and the hence the F-statistic becomes smaller. This is seen in the graph. Hence even if the Significant model was true, we would fail to detect it from the F-statistic for higher values of Sigma. We can see that the F-statistic graph for the significant model with sigma = 10 looks almost the same as the Non Significant model F-statistic for sigma = 1. For the Non-significant models we see that F-statistic doesnt change by much since the non-sgnificant model is stating the the Unexplained is high, hence the denominator in the equation above is high, hence F-statistic is low and similar irrespective of sigma.

* Now we analyze the impact of sigma on the p-value by looking at the graphs in Figure \@ref(fig:pvalcomp). For the Significant model, we assume that we know that the model is true, and in our Hypothesis test we should reject the Null Hypothesis. But we see that as the noise increases our ability to reject the Null Hypothesis decreases and the p-value graph starts to tend towards the non-significant model graph and our Type II errors increase with noise. For the non-significant models we see that p-value is uniformaly distributes as if attributed purely to chance. Hence all the graphs of p-value for Non significant model are similar.

* Based on what we have observed above, it goes to reason that $R^2$ should also decrease with noise since the variability will increase. That is what we see when we look at Figure \@ref(fig:R2comp). We see that the impact to $R^2$ is pretty severe even by a slight increase in $\sigma$. $R^2 = 1 - {\text{Sum of Squares Error} \over \text{Sum of Squares Total}} = 1 - {\sum{e^2} \over \sum{(y_i - \bar y)^2}}$. Based on the above equation it goes to reason that as the error increases due to sigma, the impact to $R^2$ is square of the error, hence more extreme. This corroborates what we are seeing in the significant model graph as we move from left to right. For the non significant model we see no impact since we are saying that the model does not explain the response hence $R^2$ is low throughout


### Distributions of the graphs
Now we analyze if the F-statistic, p-value and $R^2$ graphs follow any standard distributions. 

* Looking at Figure \@ref(fig:fstatcomp) to see if F-statistic follows a distribution, we overlay the F-distribution curve on top of each of the graphs. Now we know that a distribution will follow the F-statistic curve if the Null Hypothesis is true. We see that for the Significant model, where we know that the Null Hypothesis is rejected for $\sigma$ 1, the distribution falls towards the extreme tail of the F-curve. As the F-statistic decreases with the increase in $\sigma$, the distribution starts getting more aligned to the F-curve. For the non-significant model, where the null hypothesis that there is no relationship between predictor and response, is true all the time, the distribution follows the F-curve irrespective of $\sigma$.

* When we look at Figure \@ref(fig:pvalcomp) we see that for the significant model, the distribution starts as a single bar when $\sigma$ is 1, and then starts getting more right skewed as $\sigma$ increases. For the non significant model, we see that p-value follows a uniform distribution implying that p-value is driven by pure chance when there is no relationship. We have overlaid the uniform curve (flat line) on top of the non significant model charts.

* For Figure \@ref(fig:R2comp), we observe that when $R^2$ is high, the distribution of $R^2$ is left skewed. As the variability increases with increasing $\sigma$, as the value of $R^2$ decreases, the distribution starts getting right skewed. For the non-significant model where $R^2$ is low, the distribution is right skewed.


## Conclusion
We conclude that for a model which has a signal that is useful for prediction, where atleast one of the beta parameters has a significant linear relationship, increase in noise level or $\sigma$ decreases our ability to accurately predict. When doing a Hypothesis test, higher sigma also increases the chances of Type II errors since there is a higher probability of us not rejecting the Null Hypothesis even when the alternate Hypothesis is true


# Simulation Study 2: Using RMSE for Selection?

In homework we saw how Test RMSE can be used to select the “best” model. In this simula tion study we will investigate how well this procedure works. Since splitting the data is random, we don’t expect it to work correctly each time. We could get unlucky. But averaged over many attempts, we should expect it to select the appropriate model.

We will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 3$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.5$.

We will consider a sample size of $500$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$

Use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors. These should be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Each time you simulate the data, randomly split the data into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

For each, fit **nine** models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, the correct form of the model as noted above
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

Todo - comment from office hours - parameters for x7, x8, x9 the parameters will be 0


For each model, calculate Train and Test RMSE.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

Repeat this process with $1000$ simulations for each of the $3$ values of $\sigma$. For each value of $\sigma$, create a plot that shows how average Train RMSE and average Test RMSE changes as a function of model size. Also show the number of times the model of each size was chosen for each value of $\sigma$.

Done correctly, you will have simulated the $y$ vector $3×1000=3000$ times. You will have fit $9×3×1000=27000$ models. A minimal result would use $3$ plots. Additional plots may also be useful.

Potential discussions:

- Does the method **always** select the correct model? On average, does is select the correct model?
- How does the level of noise affect the results?


## Introduction

To be filled

## Methods

We set the seed to ensure reproducible results
```{r}
birthday = 19730321
set.seed(birthday)
```

```{r}
x_predictors_df <- read.csv("study_2.csv")

n <- 500
mu <- 0
num_simul <- 1000

beta_0 <- 0
beta_1 <- 3
beta_2 <- -4
beta_3 <- 1.6
beta_4 <- -1.1
beta_5 <- 0.7
beta_6 <- 0.5

```


```{r}
sim_data_calc_RMSE <- function(sample_size, mu, sig, num_simul, x_predictors_df, num_parameters){
  
  
  sim_train_RMSE <- rep(0,num_simul)
  sim_test_RMSE <- rep(0,num_simul)

  for (i in 1:num_simul){
    epsilon <- rnorm(sample_size, mean=mu, sd=sig)
    
    x_predictors_df$y <- beta_0 + x_predictors_df$x1*beta_1 + x_predictors_df$x2*beta_2 + x_predictors_df$x3*beta_3 + 
                          x_predictors_df$x4*beta_4 + x_predictors_df$x5*beta_5 + x_predictors_df$x6*beta_6 + epsilon
    
    
    trn_idx = sample(1:nrow(x_predictors_df), 250)
    trn <- slice(x_predictors_df, trn_idx)
    tst <- slice(x_predictors_df, -1*trn_idx)
    
    
    if(num_parameters==2){
      m <- lm(y ~ x1, data = trn)
    } else if(num_parameters==3){
      m <- lm(y ~ x1+x2, data = trn)
    } else if(num_parameters==4){
      m <- lm(y ~ x1+x2+x3, data = trn)
    } else if(num_parameters==5){
      m <- lm(y ~ x1+x2+x3+x4, data = trn)
    } else if(num_parameters==6){
      m <- lm(y ~ x1+x2+x3+x4+x5, data = trn)
    } else if(num_parameters==7){
      m <- lm(y ~ x1+x2+x3+x4+x5+x6, data = trn)
    } else if(num_parameters==8){
      m <- lm(y ~ x1+x2+x3+x4+x5+x6+x7, data = trn)
    } else if(num_parameters==9){
      m <- lm(y ~ x1+x2+x3+x4+x5+x6+x7+x8, data = trn)
    } else{
      m <- lm(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9, data = trn)
    }
    
    

    sim_train_RMSE[i] <- sqrt(sum((trn["y"]-predict(m))^2)/nrow(trn))
    sim_test_RMSE[i] <- sqrt(sum((tst["y"]-predict(m, newdata = tst))^2)/nrow(tst))
  }
  
  return(data.frame("train_RMSE"=sim_train_RMSE, "test_RMSE"=sim_test_RMSE))
}
```



```{r}
sig <- 1

m1_sig1_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 2)
m2_sig1_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 3)
m3_sig1_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 4)
m4_sig1_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 5)
m5_sig1_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 6)
m6_sig1_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 7)
m7_sig1_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 8)
m8_sig1_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 9)
m9_sig1_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 10)


sig <- 2

m1_sig2_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 2)
m2_sig2_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 3)
m3_sig2_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 4)
m4_sig2_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 5)
m5_sig2_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 6)
m6_sig2_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 7)
m7_sig2_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 8)
m8_sig2_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 9)
m9_sig2_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 10)

sig <- 4

m1_sig4_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 2)
m2_sig4_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 3)
m3_sig4_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 4)
m4_sig4_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 5)
m5_sig4_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 6)
m6_sig4_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 7)
m7_sig4_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 8)
m8_sig4_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 9)
m9_sig4_df <- sim_data_calc_RMSE(sample_size = n, mu = mu, sig = sig, num_simul = num_simul, x_predictors_df = x_predictors_df, num_parameters = 10)



```

```{r}
x=seq(1:9)


df_train_RMSE <- as.data.frame(rbind(c("Model 1", mean(m1_sig1_df$train_RMSE),mean(m1_sig2_df$train_RMSE),mean(m1_sig4_df$train_RMSE)),
                                     c("Model 2", mean(m2_sig1_df$train_RMSE),mean(m2_sig2_df$train_RMSE),mean(m2_sig4_df$train_RMSE)),
                                     c("Model 3", mean(m3_sig1_df$train_RMSE),mean(m3_sig2_df$train_RMSE),mean(m3_sig4_df$train_RMSE)),
                                     c("Model 4", mean(m4_sig1_df$train_RMSE),mean(m4_sig2_df$train_RMSE),mean(m4_sig4_df$train_RMSE)),
                                     c("Model 5", mean(m5_sig1_df$train_RMSE),mean(m5_sig2_df$train_RMSE),mean(m5_sig4_df$train_RMSE)),
                                     c("Model 6", mean(m6_sig1_df$train_RMSE),mean(m6_sig2_df$train_RMSE),mean(m6_sig4_df$train_RMSE)),
                                     c("Model 7", mean(m7_sig1_df$train_RMSE),mean(m7_sig2_df$train_RMSE),mean(m7_sig4_df$train_RMSE)),
                                     c("Model 8", mean(m8_sig1_df$train_RMSE),mean(m8_sig2_df$train_RMSE),mean(m8_sig4_df$train_RMSE)),
                                     c("Model 9", mean(m9_sig1_df$train_RMSE),mean(m9_sig2_df$train_RMSE),mean(m9_sig4_df$train_RMSE))))

colnames(df_train_RMSE) <- c("Model number", "RMSE for Sigma = 1", "RMSE for Sigma = 2", "RMSE for Sigma = 4")
df_train_RMSE$`RMSE for Sigma = 1` <- as.numeric(df_train_RMSE$`RMSE for Sigma = 1`)
df_train_RMSE$`RMSE for Sigma = 2` <- as.numeric(df_train_RMSE$`RMSE for Sigma = 2`)
df_train_RMSE$`RMSE for Sigma = 4` <- as.numeric(df_train_RMSE$`RMSE for Sigma = 4`)


df_test_RMSE <- as.data.frame(rbind(c("Model 1", mean(m1_sig1_df$test_RMSE),mean(m1_sig2_df$test_RMSE),mean(m1_sig4_df$test_RMSE)),
                                    c("Model 2", mean(m2_sig1_df$test_RMSE),mean(m2_sig2_df$test_RMSE),mean(m2_sig4_df$test_RMSE)),
                                    c("Model 3", mean(m3_sig1_df$test_RMSE),mean(m3_sig2_df$test_RMSE),mean(m3_sig4_df$test_RMSE)),
                                    c("Model 4", mean(m4_sig1_df$test_RMSE),mean(m4_sig2_df$test_RMSE),mean(m4_sig4_df$test_RMSE)),
                                    c("Model 5", mean(m5_sig1_df$test_RMSE),mean(m5_sig2_df$test_RMSE),mean(m5_sig4_df$test_RMSE)),
                                    c("Model 6", mean(m6_sig1_df$test_RMSE),mean(m6_sig2_df$test_RMSE),mean(m6_sig4_df$test_RMSE)),
                                    c("Model 7", mean(m7_sig1_df$test_RMSE),mean(m7_sig2_df$test_RMSE),mean(m7_sig4_df$test_RMSE)),
                                    c("Model 8", mean(m8_sig1_df$test_RMSE),mean(m8_sig2_df$test_RMSE),mean(m8_sig4_df$test_RMSE)),
                                    c("Model 9", mean(m9_sig1_df$test_RMSE),mean(m9_sig2_df$test_RMSE),mean(m9_sig4_df$test_RMSE))))


colnames(df_test_RMSE) <- c("Model number", "RMSE for Sigma = 1", "RMSE for Sigma = 2", "RMSE for Sigma = 4")
df_test_RMSE$`RMSE for Sigma = 1` <- as.numeric(df_test_RMSE$`RMSE for Sigma = 1`)
df_test_RMSE$`RMSE for Sigma = 2` <- as.numeric(df_test_RMSE$`RMSE for Sigma = 2`)
df_test_RMSE$`RMSE for Sigma = 4` <- as.numeric(df_test_RMSE$`RMSE for Sigma = 4`)
```



```{r RMSEcomp, fig.height = 7, fig.width = 15, fig.cap = "Comparison of RMSE change across Test and Train datasets For Sigmas 1, 2 and 4" }
df_train_RMSE$`Train or Test` <- "Training RMSE"
df_test_RMSE$`Train or Test` <- "Testing RMSE"

combined_df <- rbind(df_train_RMSE, df_test_RMSE)

g1 <- ggplot(data = combined_df, aes(x=`Model number`, y=`RMSE for Sigma = 1`, group = `Train or Test`, color=`Train or Test`)) + 
  geom_point(size=2)+
  geom_line() +
  theme(legend.position = c(.7, .9)) +
  ggtitle("RMSE changes for sigma = 1")

g2 <- ggplot(data = combined_df, aes(x=`Model number`, y=`RMSE for Sigma = 2`, group = `Train or Test`, color=`Train or Test`)) + 
  geom_point(size=2)+
  geom_line() +
  theme(legend.position = c(.7, .9)) +
  ggtitle("RMSE changes for sigma = 2")

g3 <- ggplot(data = combined_df, aes(x=`Model number`, y=`RMSE for Sigma = 4`, group = `Train or Test`, color=`Train or Test`)) + 
  geom_point(size=2)+
  geom_line() +
  theme(legend.position = c(.7, .9)) +
  ggtitle("RMSE changes for sigma = 4")

grid.arrange(g1, g2, g3, ncol=3)
  

```



```{r Sigmacomp, message = FALSE, warning = FALSE, fig.height = 10, fig.width = 15, fig.cap = "Comparison of RMSE change across as sigma changes for Training and Testing datasets" }
library(reshape2)
melted_df <- melt(combined_df)
colnames(melted_df) <- c(colnames(melted_df)[1:3], "RMSE")

g1 <- ggplot(data = melted_df[melted_df$`Train or Test`=="Training RMSE",], aes(x=`Model number`, y=`RMSE`, group = `variable`, color=`variable`)) +
  geom_point(size=2)+
  geom_line() +
  theme(legend.position = c(.7, .9)) +
  ggtitle("RMSE changes for Training dataset")

g2 <- ggplot(data = melted_df[melted_df$`Train or Test`=="Testing RMSE",], aes(x=`Model number`, y=`RMSE`, group = `variable`, color=`variable`)) +
  geom_point(size=2)+
  geom_line() +
  theme(legend.position = c(.7, .9)) +
  ggtitle("RMSE changes for Testing dataset")


grid.arrange(g1, g2, ncol=2)
  

```


# Simulation Study 3: Power

In this simulation study we will investigate the **power** of the significance of regression test for simple linear regression. 

\[
H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0
\]

Recall, we had defined the *significance* level, $\alpha$, to be the probability of a Type I error.

\[
\alpha = P[\text{Reject } H_0 \mid H_0 \text{ True}] = P[\text{Type I Error}]
\]

Similarly, the probability of a Type II error is often denoted using $\beta$; however, this should not be confused with a regression parameter.

\[
\beta = P[\text{Fail to Reject } H_0 \mid H_1 \text{ True}] = P[\text{Type II Error}]
\]

*Power* is the probability of rejecting the null hypothesis when the null is not true, that is, the alternative is true and $\beta_{1}$ is non-zero.

\[
\text{Power} = 1 - \beta = P[\text{Reject } H_0 \mid H_1 \text{ True}]
\]

Essentially, power is the probability that a signal of a particular strength will be detected. Many things affect the power of a test. In this case, some of those are:

- Sample Size, $n$
- Signal Strength, $\beta_1$
- Noise Level, $\sigma$
- Significance Level, $\alpha$

We'll investigate the first three.

To do so we will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$.

For simplicity, we will let $\beta_0 = 0$, thus $\beta_1$ is essentially controlling the amount of "signal." We will then consider different signals, noises, and sample sizes:

- $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$
- $\sigma \in (1, 2, 4)$
- $n \in (10, 20, 30)$

We will hold the significance level constant at $\alpha = 0.05$.

Use the following code to generate the predictor values, `x`: values for different sample sizes.

#Todo -genereate new values of y, fit nethose models and calculate RMSEs of those models. Do that inside a function. All these need to be done a 1000 times. Sample size is 500. Within that 500 250 is training and 250 is test. You have to generate a new set of y values every time. When you create y values, create a 500 of them and then when you do that again calculate y again.
There will always be 500 y values. Build a comparision table for comparing RMSE values
Create a scatter plot that shows how avg. RMSE (y axis) changes as a function of model size (x axis). Two plots for train and test RMSE and also how that changes through sigma


We set the seed to ensure reproducible results
```{r}
birthday = 19730321
set.seed(birthday)
```

```{r}
betas <- seq(-2, 2, .1)
sigmas <- c(1,2,4)
ns <- c(10,20,30)
mu <- 0
alpha <- .05
num_simul <- 1000
```
```{r}

sim_data_calc_Power <- function(sample_size, mu, sig, beta_1, num_simul, x_values, alpha=alpha){
  #sig=1
  #x_values = seq(0, 5, length = n)
  #beta_1=-1.9
  #sample_size=10
  
  sim_pval <- rep(0, num_simul)
  for (i in 1:num_simul){
    epsilon <- rnorm(sample_size, mean=mu, sd=sig)
    
    y <- x_values*beta_1 + epsilon
    
    m <- lm(y ~ x1, data = data.frame(y, x1=x_values))
    sim_pval[i] <- glance(m)$p.value
  }
  Power <- mean(sim_pval<alpha)
  return(Power)
}
```

```{r message=FALSE}

library(data.table)
```

```{r}

Power_df <- CJ(sigmas, ns, betas)

Power_df$Power <- rep(0, nrow(Power_df))

for(sigma in sigmas)
{
  for(n in ns)
  {
      x_values = seq(0, 5, length = n)
      for(beta in betas)
      {
        Power_df[Power_df$sigmas==sigma & Power_df$ns==n & Power_df$betas==beta]$Power <- 
                    sim_data_calc_Power(sample_size=n, mu=mu, sig=sigma, beta_1=beta, num_simul=num_simul, x_values=x_values, alpha=alpha)
    }
  }
}


```


```{r Powercomp, message = FALSE, warning = FALSE, fig.height = 7, fig.width = 20, fig.cap = "Comparison of Power curves for diffrent sample sizes. Sigma increases from left to right"}

g1 <- ggplot(data = Power_df[Power_df$sigmas==1], aes(x=betas, y=Power, group=as.factor(ns), color=as.factor(ns))) +
  geom_point(size=2) +
  geom_line() +
  theme(legend.position = c(.9, .5)) +
  ggtitle("Power curves for different values of sample size for Sigma 1 ") + 
  labs(color = "Sample Size") +
  scale_color_manual(values=c('red','blue','violet'))

g2 <- ggplot(data = Power_df[Power_df$sigmas==2], aes(x=betas, y=Power, group=as.factor(ns), color=as.factor(ns))) +
  geom_point(size=2) +
  geom_line() +
  theme(legend.position = c(.9, .5)) +
  ggtitle("Power curves for different values of sample size for Sigma 2 ") + 
  labs(color = "Sample Size") +
  scale_color_manual(values=c('red','blue','violet'))

g3 <- ggplot(data = Power_df[Power_df$sigmas==4], aes(x=betas, y=Power, group=as.factor(ns), color=as.factor(ns))) +
  geom_point(size=2) +
  geom_line() +
  theme(legend.position = c(.9, .5)) +
  ggtitle("Power curves for different values of sample size for Sigma 4 ") + 
  labs(color = "Sample Size") +
  scale_color_manual(values=c('red','blue','violet'))

grid.arrange(g1, g2, g3, ncol=3)

```


Create three plots, one for each value of $\sigma$. Within each of these plots, add a “power curve” for each value of $n$ that shows how power is affected by signal strength, $\beta_1$.

Potential discussions:

- How do $n$, $\beta_1$, and $\sigma$ affect power? Consider additional plots to demonstrate these effects.
- Are $1000$ simulations sufficient?
